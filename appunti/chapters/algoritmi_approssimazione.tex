\section{Introduzione}
% Recap problemi di ottimizzazione, pag 42.

\subsection{Problemi di ottimizzazione}
% pag 5

I problemi di ottimizzazione sono definiti su insiemi di istanze e soluzioni $
\bi, \bs
$ per cui esiste una funzione di costo
% In un problema di ottimizzazione, si definisce una funzione di costo associato ad una soluzione:
\begin{equation*}
    c : \bs{} \to \mathbb{R}
\end{equation*}
e, dato l'insieme di soluzioni ammissibili associate ad un'istanza
\begin{equation*}
    \bs{} (i) = \left\{ s \in \bs{} : i \, \bpi{} \, s \right\}
\end{equation*}
si vuole individuare la soluzione di costo massimo (o minimo)
\begin{equation*}
    s_{i}^{*} = \argmax \left\{ c(s) : s \in \bs{} (i) \right\}
\end{equation*}

\subsection{Approcci risolutivi}

\subsubsection{Risoluzioni esaustive}

Si cerca la soluzione esatta, con una ricerca esaustiva efficiente, per esempio con le tecniche \emph{Branch and Bound} o \emph{Branch and Cut}.

\subsubsection{Algoritmi pseudo polinomiali}

Per alcuni problemi, se i dati fossero rappresentati con codifica unaria, l'algoritmo risolutivo sarebbe di complessità polinomiale.

I problemi Strong-NP-Hard restano NP-hard anche sotto encoding unario.

\emph{Subset Sum}, per esempio, si può approcciare come problema di programmazione dinamica (ESERCIZIO).
Si
% definiscono
possono infatti definire 
i sottoproblemi $S_{i, j}$ dove $S_i$ è un prefisso di $S$: $ S_i = \{ s_1, \dots, s_i \} $
e dove $1 \leq j \leq t$.
Questi sottoproblemi sono
% Quanti sono questi sottoproblemi? Ce ne sono
$n \, t$, ma $t$ è il numero nell'istanza,
% quanti bit ci sono per rappresentarlo?
che si rappresenta con solamente $\log t$ bit.
Il numero di problemi è quindi esponenziale nella taglia dell'istanza, se $t = 2^{50}$, si genererebbe un numero enorme di problemi, ma il numero viene rappresento con $50$ bit.
La taglia è quindi logaritmica in $t$ ma il numero di problemi è lineare in $t$.
Il numero di problemi sarebbe polinomiale sotto la codifica unaria.
In istanze ragionevoli dove $t$ è piccolo (sul miliardo) si possono risolvere.
\\
L'algoritmo di programmazione dinamica usa una proprietà di sottostruttura che permette di trovare la soluzione $S_{i,j}$ a partire da istanze precedenti $S_{i-1,j}$ con valori di $j$ più piccoli.

Molte istanze piccole di problemi pseudo polinomiali vengono risolte usando la programmazione dinamica.

\subsubsection{Rinunciare all'ottimalità}

Si rinuncia all'ottimalità e si ottiene una soluzione che ha una relazione garantita con la soluzione ottima. Si può \emph{quantificare} di quanto sia peggiore dell'ottimo.

\section{Algoritmi di approssimazione}

\subsection{Definizione}

% Definizione algoritmo di rho-approssimazione, pag 43.
% TODO
\begin{definition}[Algoritmo di approssimazione]
    \label{def:algoapprossimazione}
    Dato $\bpi$ di ottimizzazione,
    $A_{\bpi}$
    è un algoritmo per
    $\bpi$
    che ritorna $
    A_{\bpi} (i) \in \bs (i)
    $
    Si dice che $A_{\bpi}$ è di
    $\rho (n)$-approssimazione
    per $\bpi$ se $\forall i \in \bi, |i|=n$, vale, per $\rho(n) \geq 1$
    \begin{itemize}
        \item problema di \texttt{minimo:} $
            \displaystyle
            \frac{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }{
                c \left( 
                    s^* \left( i \right)
                \right)
            } \leq \rho \left( n \right)
            $
        \item problema di \texttt{massimo:} $
            \displaystyle
            \frac{
                c \left( 
                    s^* \left( i \right)
                \right)
            }{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            } \leq \rho \left( n \right)
            $
    \end{itemize}
    Dove $
        c \left( 
            s^* \left( i \right)
        \right)
    $ è il costo della soluzione ottima.
    Nota: se $A_{\bpi}$ risolve il problema, $\rho(n)=1$.
    Si può riscrivere la maggiorazione in una singola espressione:
    \begin{equation*}
        \max 
        \left\{ 
            \frac{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }{
                c \left( 
                    s^* \left( i \right)
                \right)
            }
            ,
            \frac{
                c \left( 
                    s^* \left( i \right)
                \right)
            }{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }
        \right\}
        \leq \rho \left( n \right)
    \end{equation*}
\end{definition}

\subsubsection{Lower/Upper bound sul costo ottimo}

È interessante notare che un algoritmo di approssimazione fornisca in tempo polinomiale un lower (upper) bound al costo della soluzione ottima, che non è conoscibile in tempo ragionevole.
\begin{itemize}
    \item problema di \texttt{minimo:} $
        \displaystyle
            c \left( 
                s^* \left( i \right)
            \right)
            \geq
            \frac{
                c \left( 
                    A_{\bpi} \left( i \right)
                \right)
            }{
                \rho \left( n \right)
            }
        $
    \item problema di \texttt{massimo:} $
        \displaystyle
            c \left( 
                s^* \left( i \right)
            \right)
        \leq
        \rho \left( n \right)
        \cdot
            c \left( 
                A_{\bpi} \left( i \right)
            \right)
        $
\end{itemize}
% TODO commento pag 44.2 su relazione s* s'

\subsubsection{Tipologie di algoritmi}

La 
$\rho (n)$-approssimazione
è un concetto generale, legato alla taglia dell'istanza.

I problemi possono essere approssimati con qualità molto variabile.

Per esempio,
per \emph{Vertex Cover}, si trova un'approssimazione $\rho(n) = 2$ costante.
Per \emph{Set Cover}, la qualità dell'approssimazione è legata alla taglia $\rho(n) = \Theta ( \log n )$.
Per il \emph{Travelling Salesman Problem} in versione generale, e per \emph{Clique}, sotto l'ipotesi $\bp \ne \bnp$, si trova un limite inferiore all'approssimazione $\rho(n) = \Omega ( n^{1-\varepsilon} )$.

\subsection{Schemi di approssimazione}

\begin{definition}[Schema di approssimazione]
    \label{def:schemaapprox}
    L'algoritmo
    $A_{\bpi} (i,\varepsilon)$
    è uno schema di approssimazione
    per $\bpi$ se
    $A_{\bpi} (i,\varepsilon)$
    è di $(1+\varepsilon)$-approssimazione per $\bpi$
\end{definition}

\begin{definition}[PTAS]
    \label{def:ptas}
    L'algoritmo
    $A_{\bpi} (i,\varepsilon)$
    è uno schema di approssimazione polinomiale
    (\emph{polinomial time approximation scheme})
    per $\bpi$ se
    $A_{\bpi} (i,\varepsilon)$
    è di $(1+\varepsilon)$-approssimazione per $\bpi$
    e, fissato $\varepsilon$, ha complessità polinomiale.
\end{definition}

\begin{definition}[FPTAS]
    \label{def:fptas}
    L'algoritmo
    $A_{\bpi} (i,\varepsilon)$
    è uno schema di approssimazione pienamente polinomiale
    (\emph{fully polinomial time approximation scheme})
    per $\bpi$ se
    $A_{\bpi} (i,\varepsilon)$
    è di $(1+\varepsilon)$-approssimazione per $\bpi$
    e, fissato $\varepsilon$, è polinomiale sia in $n$ sia in $1/\varepsilon$.
\end{definition}

La complessità è quindi legata alla taglia e al fattore di approssimazione scelto, e può essere di varie forme, per esempio:
\\
Algoritmi con complessità legata in maniera polinomiale ad $\varepsilon$ e alla taglia $n$ (FPTAS):
% \begin{equation*}
    % T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    % O \left( 
        % \frac{1}{\varepsilon^2} \, n^3
    % \right)
% \end{equation*}
% In questo caso, se si vuole ridurre l'errore relativo di un fattore costante, la complessità cresce di un fattore costante
% \begin{equation*}
    % \frac{1}{\varepsilon^2}
    % \leadsto
    % \frac{1}{\varepsilon^2} \, k^2
% \end{equation*}
% ooooooooooooooooooooooo
\begin{align*}
    T_{A_{\bpi}} \left( n, \varepsilon \right) &= 
    O \left( 
        \frac{1}{\varepsilon^2} \, n^3
    \right)
    \intertext{se si varia l'errore relativo di un fattore $k$ costante, la complessità cresce di un fattore costante}
    \rho = (1+\varepsilon)
    &
    \leadsto
    \rho = \left( 
        1+\frac{\varepsilon}{k}
    \right)
    \\
    T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    O \left( 
        \frac{1}{\varepsilon^2} \, n^3
    \right)
    &
    \leadsto
    O \left( 
        \frac{1}{\varepsilon^2} \, k^2 \, n^3
    \right)
\end{align*}
Altri algoritmi vedono comparire $\varepsilon$ come esponente, in questo caso la complessità \emph{è} polinomiale per $\varepsilon$ fissato, ma cresce rapidamente variando $\varepsilon$ (PTAS):
\begin{align*}
    T_{A_{\bpi}} \left( n, \varepsilon \right) &= 
    O \left( 
        % n^{\frac{1}{\varepsilon}}
        n^{1 / \varepsilon}
    \right)
    \intertext{se si varia l'errore relativo di un fattore $k$}
    \rho = (1+\varepsilon)
    &
    \leadsto
    \rho = \left( 
        1+\frac{\varepsilon}{k}
    \right)
    \\
    T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    O \left( 
        n^{1 / \varepsilon}
    \right)
    &
    \leadsto
    O \left( 
        n^{1 / (\varepsilon/k)}
    \right)
    =
    O \left( 
        \left( n^{1/\varepsilon} \right)^k
    \right)
\end{align*}
Spesso si ottiene una complessità della forma
\begin{equation*}
    T_{A_{\bpi}} \left( n, \varepsilon \right) = 
    O \left( 
        n^h \, 2^{(1/\varepsilon)^k}
    \right)
\end{equation*}
quando si generano in tempo polinomiale delle sottoistanze piccole (di dimensione legata a $\varepsilon$), che poi si risolvono esaustivamente.

\section{Vertex cover}
% pag 45-48

\subsection{Introduzione}

Dato un grafo $G=(V,E)$ non orientato, si vuole identificare il \emph{Vertex Cover} di taglia minima, ovvero un sottoinsieme di vertici $V^* \subseteq V$ tale che ogni arco in $E$ ha almeno un estremo in $V^*$.

\subsubsection{Approccio Greedy}

Un primo approccio greedy potrebbe essere quello di scegliere un nodo arbitrario, eliminare gli archi coperti e iterare fino a coprire tutti gli archi.
Per provare che un algoritmo approssima male il problema, è sufficiente esibire un singolo controesempio.
In questo caso se si considera una stella, l'algoritmo potrebbe selezionare tutti gli estremi e non in centro, generando un $VC$ di taglia $n-1$, rispetto alla scelta ottima del singolo nodo centrale.
Il fattore di approssimazione risulta $
\rho (n) = 
(n-1)/1 = n-1
$ che è quasi il peggiore possibile.
Si può pensare di seguire una scelta greedy più astuta, per esempio scegliendo il nodo di grado massimo, ma in questo caso si può provare che risulta un algoritmo con fattore di approssimazione $
\rho (n) = \log n
$.

\subsubsection{Approccio tramite costrutto polinomiale}

Si segue quindi una strada differente: basandosi su qualche costrutto che si riesce a costruire in tempo polinomiale, si ottiene un \emph{Vertex Cover}, e ne si studia la dimensione relativa all'ottimo.

Per un algoritmo di approssimazione vanno studiati
% \begin{itemize}
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item correttezza
    \item complessità
    \item fattore di approssimazione
\end{itemize}

\subsection{Approssimazione tramite \emph{matching} massimale}

Il \emph{matching}, o \emph{independent edge set}, è un insieme di archi senza vertici in comune.
A partire da un \emph{matching} massimale $M$ (ovvero per cui se viene aggiungo un arco, non è più un \emph{matching} valido), si costruisce un \emph{Vertex Cover} che consiste in tutti gli estremi degli archi in $M$.
\begin{algorithm}[H]
\caption{Aprossimatore per Vertex Cover}\label{alg:approxvc}
\begin{algorithmic}[1]
    \Procedure{Approx\_VC}{$G=(V,E)$}
        \State $V' \gets \emptyset$
        \State $E' \gets E$
        \While{$E' \ne \emptyset$}
            \State * sceglie arco arbitrario $\{ u,v \} \in E$ *
            \State $V' \gets V' \cup \{ u,v \}$
            \label{alg:approxvc:vvuv}
            \State $E' \gets E' - \{ 
                e \in E' : \exists z \in V :
                ( e = \{ u,z \} )
                \vee
                ( e = \{ v,z \} )
            \}
            \label{alg:approxvc:cleanup}
            $
        \EndWhile
        \State return $V'$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\noindent
Nota: alla riga \ref{alg:approxvc:vvuv}, $V'$ è un insieme di nodi a cui vengono aggiungi i due nodi nell'arco, che è visto come insieme di due nodi.

\subsubsection{Complessità}

La complessità di questo algoritmo è lineare, $O ( |V| + |E| )$.

\subsubsection{Correttezza}

Per la correttezza, va argomentato che $V' = VC$. Questo è vero, infatti si esce dal ciclo solo quando $E'$ è vuoto, quindi ogni arco $e=\{u,v\}$ viene eliminato, per uno di due motivi:
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item è l'arco scelto arbitrariamente, quindi i suoi nodi sono inseriti in $V'$ (riga  \ref{alg:approxvc:vvuv})
    \item uno dei suoi estremi è parte dell'arco preso in considerazione, ed $e$ viene rimosso nel clean up (riga \ref{alg:approxvc:cleanup})
\end{itemize}
In entrambi i casi, almeno uno degli estremi di ogni arco è in $V'$, che è quindi un \emph{Vertex Cover}.

\subsubsection{Fattore di approssimazione}
% pag 46
Il fattore di approssimazione è dato da
\begin{equation*}
    \rho =
    \frac{
    |V'|
    }{
    |V^*|
    }
    =
    \frac{
        \text{costo soluzione ammissibile ritornata}
    }{
        \text{costo soluzione ottima}
    }
\end{equation*}
di cui si vuole trovare un limite superiore.

Per quanto riguarda $|V'|$:
sia $M \subseteq E$ l'insieme degli archi selezionati da $APPROX\_VC$.
$M$ è un \emph{matching}, per cui nessuna coppia di archi selezionati condivide estremi (gli archi sono insiemi di nodi):
\begin{equation*}
    \forall e_1, e_2 \in M
    \to
    e_1 \cap e_2 = \emptyset
\end{equation*}
Gli archi selezionati sono quindi tutti indipendenti, e la taglia del $VC$ selezionato $V'$ è esattamente il doppio della taglia del \emph{matching}
(vengono scelti entrambi gli estremi di ogni arco in $M$)
\begin{equation*}
    |V'| = 2 |M|
\end{equation*}
Nota: questo è un valore molto preciso del valore della funzione obiettivo che l'algoritmo ritorna, è il costo della soluzione ammissibile ritornata.

Per quanto riguarda $|V^*|$:
va trovato un \emph{lower bound} alla taglia del $VC$ ottimo $
|V^*|
$ rispetto a $|M|$ (si cerca un \emph{upper bound} di $\rho$).
Un \emph{matching} qualsiasi è un insieme di archi che non ha estremi in comune.
Un qualsiasi \emph{Vertex Cover} per un grafo con questo \emph{matching}, tutti gli archi devono essere controllati.
Per controllare $|M|$ archi disgiunti, serve almeno un nodo per arco.
Questo vale per ogni $VC$ costruito su un qualsiasi \emph{matching}, e in particolare vale per il $VC$ ottimo.
% Gli archi di $M$ sono disgiunti, per cui in ogni $VC$ deve selezionare almeno un nodo per arco:
\begin{equation*}
    |V^*| \geq |M|
\end{equation*}
Combinando i due risultati
\begin{equation*}
    \rho =
    \frac{
    |V'|
    }{
    |V^*|
    }
    \leq
    \frac{
    2|M|
    }{
    |M|
    }
    = 2
\end{equation*}
% C'è una relazione molto forte tra la taglia di un \emph{matching} e il VC.

In più, il \emph{matching} trovato è massimale, per cui se gli venisse aggiunto un arco, non sarebbe un \emph{matching} valido: 
\begin{equation*}
    \nexists e \in E - M : M \cup \left\{ e \right\} \text{ è un \emph{matching}}
\end{equation*}
ossia è uno dei più grandi \emph{matching} possibili.
Se l'arco $e$ esistesse, l'algoritmo non sarebbe terminato, perché  per eliminare tutti gli archi, devono avere almeno un estremo che appartiene al \emph{matching}.

Qualsiasi \emph{matching} avrebbe dato un \emph{lower bound}, il \emph{matching} massimale serve per provare l'upper bound. Se non fosse massimale, l'unione dei nodi degli archi che compongono il \emph{matching} non sarebbe un \emph{Vertex Cover}.

\subsubsection{Analisi \emph{slack} o \emph{tight}}
%pag 47

Questa analisi è \emph{slack} o \emph{tight}? È un'analisi che non è la migliore possibile, o meglio di così questo algoritmo non può fare?
Ovvero, \emph{l'analisi} dell'algoritmo può essere migliorata, rendendo per esempio più stretti i \emph{bound} trovati?

Si determina un grafo sotto cui l'algoritmo di approssimazione performa il peggio possibile e ottiene proprio il fattore di approssimazione.

Per esempio il grafo in figura \ref{fig:vcapprox} ha un \emph{Vertex Cover} di taglia minima $3$, che si può verificare con una ricerca esaustiva data la piccola taglia.

Applicando l'algoritmo di approssimazione, e scegliendo volontariamente gli archi in un ordine sfortunato, però, risulta un \emph{Vertex Cover} da $6=2 \cdot 3$ elementi, per cui l'approssimazione non è migliorabile (figura \ref{fig:vcapproxbad}).
\begin{figure}[h]
    \centering
    \caption{Esempio di esecuzione pessima dell'algoritmo di approssimazione.
        Gli archi blu sono quelli presi in esame all'iterazione corrente, gli archi rossi sono quelli coperti dai nodi selezionati.
        I nodi blu sono quelli selezionati in $V'$.
    }
    \label{fig:vcapproxbad}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{$VC$ ottimo di taglia 3 (nodi verdi)}
        \label{fig:vcapprox}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=green!24},
                edge/.style = {draw,thick,-},
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                    {(0,0)/A}, {(0,2)/B}, {(2,2)/C}, {(4,2)/D},
                    {(2,0)/E}, {(4,0)/F}, {(6,2)/G}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest in {
                    A/B, B/C, C/E, C/D, E/F, D/E, D/G, F/D}
                \path[edge] (\source) -- (\dest);
            % color a node
            \foreach \vertex in {B, D, E}
                \path node[selected vertex] at (\vertex) {$\vertex$};
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Primo arco: $V' = \left\{ B,C \right\}$}
        \label{fig:vcapproxbad1}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=blue!24},
                edge/.style = {draw,thick,-},
                selected edge/.style = {draw,line width=5pt,-,blue!35},
                ignored edge/.style = {draw,line width=5pt,-,red!25}
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                    {(0,0)/A}, {(0,2)/B}, {(2,2)/C}, {(4,2)/D},
                    {(2,0)/E}, {(4,0)/F}, {(6,2)/G}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest in {
                    A/B, B/C, C/E, C/D, E/F, D/E, D/G, F/D}
                \path[edge] (\source) -- (\dest);
            % color a node
            \foreach \vertex in {B, C}
                \path node[selected vertex] at (\vertex) {$\vertex$};
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {B/C}
                    \path[selected edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {A/B,C/E,C/D}
                    \path[ignored edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
    \\[2pt]
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Secondo arco: $V' = \left\{ B,C,E,F \right\}$}
        \label{fig:vcapproxbad2}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=blue!24},
                edge/.style = {draw,thick,-},
                selected edge/.style = {draw,line width=5pt,-,blue!35},
                ignored edge/.style = {draw,line width=5pt,-,red!25}
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                    {(0,0)/A}, {(0,2)/B}, {(2,2)/C}, {(4,2)/D},
                    {(2,0)/E}, {(4,0)/F}, {(6,2)/G}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest in {
                    A/B, B/C, C/E, C/D, E/F, D/E, D/G, F/D}
                \path[edge] (\source) -- (\dest);
            % color a node
                \foreach \vertex in {B, C, E, F}
                \path node[selected vertex] at (\vertex) {$\vertex$};
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {E/F}
                    \path[selected edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {B/C,A/B,C/E,C/D,D/E,D/F}
                    \path[ignored edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Terzo arco: $V' = \left\{ B,C,E,F,D,G \right\}$}
        \label{fig:vcapproxbad3}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=blue!24},
                edge/.style = {draw,thick,-},
                weight/.style = {font=\small},
                selected edge/.style = {draw,line width=5pt,-,blue!35},
                ignored edge/.style = {draw,line width=5pt,-,red!25}
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                    {(0,0)/A}, {(0,2)/B}, {(2,2)/C}, {(4,2)/D},
                    {(2,0)/E}, {(4,0)/F}, {(6,2)/G}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest in {
                    A/B, B/C, C/E, C/D, E/F, D/E, D/G, F/D}
                \path[edge] (\source) -- (\dest);
            % color a node
            \foreach \vertex in {B, C, E, F, D, G}
                \path node[selected vertex] at (\vertex) {$\vertex$};
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {D/G}
                    \path[selected edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {B/C,E/F,A/B,C/E,C/D,D/E,D/F}
                    \path[ignored edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
\end{figure}

\subsubsection{Approssimazione di $CLIQUE$}

Seguendo la funzione trovata nella riduzione da $CLIQUE$ a $VC$, si potrebbe pensare di utilizzare il risultato ottenuto per identificare una Clique nel grafo.

\begin{algorithm}[H]
\caption{Approssimatore per Clique}\label{alg:approxclique}
\begin{algorithmic}[1]
    \Procedure{Approx\_C}{$G=(V,E)$}
        \State $G^C = (V,E^C)$
        \State $V' \gets $ \Call{Approx\_VC}{$G^C$}
        \State return $V - V'$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
    \centering
    \caption{Trasformazione da \emph{Vertex Cover} a Clique}
    \label{fig:approxclique}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \caption{Grafo originale}
        \label{fig:approxclique1}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                    {(0,0)/A}, {(0,2)/B}, {(2,2)/C}, {(2,0)/D}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest in {
                    A/B, B/C, C/D, B/D}
                \path[edge] (\source) -- (\dest);
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \caption{$VC$ in $G^C$}
        \label{fig:approxclique2}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=green!24},
                edge/.style = {draw,thick,-},
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                    {(0,0)/A}, {(0,2)/B}, {(2,2)/C}, {(2,0)/D}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest in {
                    A/C, A/D}
                \path[edge] (\source) -- (\dest);
            % color a node
            \foreach \vertex in {A}
                \path node[selected vertex] at (\vertex) {$\vertex$};
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \caption{Clique in $G$}
        \label{fig:approxclique3}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=blue!24},
                edge/.style = {draw,thick,-},
            ]
            % First we draw the vertices
            \foreach \pos/\name in {
                    {(0,0)/A}, {(0,2)/B}, {(2,2)/C}, {(2,0)/D}}
                \node[vertex] (\name) at \pos {$\name$};
            % Connect vertices with edges and draw weights
            \foreach \source/ \dest in {
                    A/B, B/C, C/D, B/D}
                \path[edge] (\source) -- (\dest);
            % color a node
            \foreach \vertex in {B, C, D}
                \path node[selected vertex] at (\vertex) {$\vertex$};
        \end{tikzpicture}
    \end{subfigure}
\end{figure}
Questo vale se il \emph{Vertex Cover} trovato è di dimesione massima. Se però si considera un'istanza di $CLIQUE$ $G=(V,E)$,
per cui la clique massima ha dimensione $|V^*|=n/2+1$, il $VC$ nel complementato avrà taglia $n-\left( n/2 +1 \right) = n/2 -1$. L'algoritmo di approssimazione sbaglia al massimo di un fattore 2, per cui $|V'|=2 \, \left( n/2 -1 \right) = n-2$. La clique trovata ha taglia $|V-V'|=n-\left( n-2 \right) = 2$. Il fattore di approssimazione risulta allora
\begin{equation*}
    \rho_{AC} \geq \frac{n/2+1}{2} \approx \frac{n}{4}
\end{equation*}
Questa cattiva prestazione dell'algoritmo deriva dal fatto che la trasformazione non preserva l'approssimazione.

% TODO pag 48 disegni strani delle riduzioni

\section{Travelling Salesman Problem}
% pag 48
Si ricordano le definizioni dei problemi di $HAMILTON$ e $TSP$:
\begin{align*}
    \text{Hamilton:} & \quad \\
    \texttt{istanza:} \quad & \langle G=(V,E) \rangle \\
    \texttt{domanda:} \quad & G \text{ contiene un ciclo semplice che tocca tutti i nodi (\emph{tour})?} \\
    \text{TSP:} & \quad \\
    \texttt{istanza:} \quad & \langle G_C=(V,E_C), c, k \rangle \\
    \text{dove} \quad & G_C \text{ grafo completo non orientato} \\
    & c : V \times V = E \to \mathbb{N} \\
    & k \in \mathbb{N} \\
    \texttt{domanda:} \quad & G \text{ contiene un ciclo \emph{Hamiltoniano} di costo $\leq k$?} \\
    \text{dove} \quad &  c(\langle v_1, \cdots, v_{|V|+1} \rangle) = \sum_{i=1}^{|V|} c(v_i, v_{i+1} )  \\
\end{align*}

\subsection{Inapprossimabilità del $TSP$ generale}
% pag 49
Se $\bp = \bnp$ sarebbe disponibile la soluzione esatta.

Sotto l'ipotesi $\bp \ne \bnp$, si mostra che se $TSP$ fosse $\rho(n)$-approssimabile in tempo polinomiale, si potrebbe risolvere un problema $\bnpc$ in tempo polinomiale, che è assurdo.

\begin{theorem}[Inapprossimabilità del $TSP$]
    \label{teo:inapprossimabilitatsp}
    Sotto l'ipotesi $\bp \ne \bnp$,
    data un'istanza di $TSP$
    $
        \langle
            G = (V,E), c
        \rangle
    $, per qualsiasi
    $
        \rho (|V|)
    $ calcolabile in tempo polinomiale,
    allora il $TSP$ non è $
        \rho (|V|)
    $-approssimabile in tempo polinomiale.
\end{theorem}
Nota: per alleggerire la notazione, $\rho \equiv \rho(|V|)$
\begin{proof}
    Per assurdo, esista un algoritmo $
    A_{TSP} ( \langle G = c \rangle)
    $ di
    $\rho$-approssimazione, poliomiale.
    Si può usare questo algoritmo per risolvere $HAMILTON$.
    Si trasforma un'istanza di $HAMILTON$ in una di $TSP$, che può essere mandata in input ad $A_{TSP}$, in maniera simile alla riduzione già vista.
    \begin{equation*}
        f :
        \langle
            G = (V,E)
        \rangle
        \to
        \langle
            G^K = (V,E^K), c
        \rangle
    \end{equation*}
    dove il grafo $G^K$ è il grafo completato, e i costi sono associati agli archi nella maniera seguente:
    \begin{equation*}
         c(e) = 
        \begin{cases}
            1 & e \in E \\
            \rho \cdot |V| & e \notin E
        \end{cases}
    \end{equation*}
    Questa funzione è calcolabile in tempo polinomiale, infatti
    \begin{itemize}[parsep=0pt,partopsep=0pt,topsep=2pt]
        \item la taglia della nuova istanza è polinomiale, il grafo completo ha numero di archi quadratico
        \item i costi sono legati a $\rho$, che è polinomiale per ipotesi, $|\rho| = poly (|V|)$
    \end{itemize}

    ``$\Rightarrow$''
    Se $ \langle G = (V,E) \rangle \in HAMILTON$, esiste in $G$ un cammino Hamiltoniano, che esiste anche in $G^K$ e ha costo $|V|$, perché tutti gli archi utilizzati sono originali.
    \\
    $A_{\rho} ( \langle G^K ,  c \rangle)$ deve ritornare un tour che non costi più di $\rho \cdot |V|$, perché è di $\rho $-approssimazione.
    Questo costo è inferiore al costo di ogni arco spurio, quindi l'algoritmo non può selezionarne nessuno.
    \\
    $A_{\rho} ( \langle G^K ,  c \rangle)$ ritorna quindi un circuito Hamiltoniano di $G$, di costo $ \leq \rho \cdot |V| + 1 $.

    ``$\Leftarrow$''
    L'algoritmo deve anche riconoscere quando un'istanza è negativa.
    \\
    $ \langle G = (V,E) \rangle \notin HAMILTON$ implica che ogni tour in $G^K$ deve usare almeno un arco in $E^K-E$.
    Allora, per ogni tour $\pi$, $c(\pi) \geq 
    \rho \cdot |V|
    + 1
    $.
    Ossia 
    $A_{\rho} ( \langle G^K ,  c \rangle)$ ritorna un tour di costo $ \geq \rho \cdot |V| + 1 $, e quindi, combinando i due risultati:
    \begin{equation*}
        \langle G \rangle \in HAMILTON
        \Leftrightarrow
        A_{\rho} ( f ( \langle G \rangle) ) \text{ ritorna un tour di costo } \leq \rho \cdot |V| + 1 
    \end{equation*}
    Sia $A_{\rho}$ sia $f$ sono polinomiali, quindi la loro composizione è polinomiale, e si è esibito un decisore per $HAMILTON$ polinomiale, che è in contrasto con l'ipotesi $\bp \ne \bnp$.
\end{proof}

\subsection{Risultati di inapprossimabilità}
% pag 50
L'idea del procedimento da seguire per provare l'inapprossimabilità di un problema è quella di creare un \emph{gap} tra il costo ottimo di istanze \emph{associate} a istanze positive e negative di un problema $\bnpc$.

Sia $\bpi_{m} \subseteq \bi \times \bs$ un problema di minimo di cui si vuole provare l'inapprossimabilità e $\bpi_{d} \in \bnpc$ un problema decisionale.
\\
Si deve trovare una funzione calcolabile in tempo polinomiale $f(x)$ che trasforma istanze di 
$\bpi_{d}$
in istanze di
$\bpi_{m}$,
tale che esiste una funzione $k(n)$ per cui
\begin{itemize}
    \item se $x \in L_{\bpi_{d}} \Rightarrow 
        c \left( 
        s^* \left( 
        f (x) \right)\right)
        \leq k \left( 
        | f(x) |
        \right)
        $
    \item se $x \notin L_{\bpi_{d}} \Rightarrow 
        c \left( 
        s^* \left( 
        f (x) \right)\right)
        > k \left( 
        | f(x) |
        \right)
        $
\end{itemize}
Studiando il costo associato alla soluzione ottima dell'istanza di 
$\bpi_{m}$,
$f(x) \in L_{ \bpi_{m} }$
si riuscirebbe a decidere un problema $\bnpc$ in tempo polinomiale.

\section{Triangle TSP}
% pag 52

\subsection{Disuguaglianza triangolare}
% pag 51

Il problema generale del $TSP$ non è di grande utilità pratica, spesso un grafo è immerso in una metrica euclidea per cui gli archi soddisfano la disuguaglianza triangolare, ovvero i costi degli archi non possono essere arbitrari.

\begin{definition}[Disuguaglianza triangolare]
    \label{def:disuguaglianzatriangolare}
    Una funzione di costo del tipo
    $c (u,v)$
    intera, posivita, simmetrica,
    soddisfa la disuguaglianza triangolare se
    \begin{equation*}
        \forall u,v,z \in V
        \Rightarrow
        c(u,v) \leq c(u,z) + c(z,v)
    \end{equation*}
\end{definition}

Anche altri spazi metrici hanno questa proprietà, per esempio lo spazio metrico indotto dai cammini minimi in un grafo.

\begin{definition}[Shortcutting]
    \label{def:shortcutting}
    In un grafo completo dove vale la disuguaglianza triangolare, si può modificare un cammino
    \begin{equation*}
        \pi = \langle \cdots, u, z, v, \cdots \rangle
        \leadsto
        \pi' = \langle \cdots, u, v, \cdots \rangle
    \end{equation*}
    e vale
    \begin{equation*}
        c (\pi') \leq c(\pi)
    \end{equation*}
    Questa proprietà si dice \emph{shortcutting} (scorciatoia).
\end{definition}

\subsection{Riduzione da TSP a TRIANGLE\_TSP}
% pag 52
Si modifica il problema del $TSP$ nel problema
$TRIANGLE\_TSP$:

\begin{align*}
    \text{TRIANGLE\_TSP:} & \quad \\
    \texttt{istanza:} \quad & \langle G=(V,E^K), c, k \rangle \\
    \text{dove} \quad & G \text{ grafo completo non orientato} \\
    & c : V \times V \to \mathbb{N} \text{ soddisfa la disuguaglianza triangolare}\\
    & k \in \mathbb{N} \\
    \texttt{domanda:} \quad & G \text{ contiene un ciclo \emph{Hamiltoniano} di costo $\leq k$?}
\end{align*}
Questa è una restrizione di un problema $\bnpc$, è stato ristretto così tanto da diventare polinomiale?
\begin{proof}[Dimostrazione $TSP \lp TRIANGLE\_TSP$]
    La funzione $f$ trasforma istanze del $TSP$ generale in istanze in cui la funzione costo rispetta la disuguaglianza triangolare scalando tutti i costi di una costante additiva.
    \begin{align*}
        f :
        \langle
        G = (V,E^K), c, k
        \rangle
        \to
        &
        \langle
        G = (V,E^K), c', k'
        \rangle
        \\
        W
        % &= 
        =
        \max_{e \in E^K} \left\{ c \left( e \right) \right\}
        ,
        \quad
        % \\
        \forall e \in E^K
        \quad
        &
        c'(e)
        =
        c(e) + W
    \end{align*}
    Questa funzione 
    \begin{itemize}
        \item è calcolabile in tempo polinomiale: $W$ è parte dell'istanza (che è polinomiale), e si somma il valore su $n^2$ archi.
        \item soddisfa la disuguaglianza triangolare:
            \begin{align*}
                \forall u,v,z
                \quad
                c'(u,v)
                &= 
                c(u,v) + W
                \\
                & \leq
                W + W
                \\
                & \leq
                c(u,z) + W + c(z,v) + W
                \\
                & \leq
                c'(u,z) + c'(z,v)
            \end{align*}
        \item è una riduzione:
            ogni tour di costo $T$ in
            $G=(V,E^K)$
            sotto $c$ diventa un tour di costo $T + |V| W$ in 
            $G=(V,E^K)$ sotto $c'$ e viceversa.
            Ogni tour infatti ha esattamente $|V|$ archi, quindi a prescindere dai loro costi singoli il costo del tour varia di $|V|W$.
            $G$ ha un tour di costo $\leq k$ sotto $c$
            se e solo se 
            $G$ ha un tour di costo $\leq k +|V|W$ sotto $c'$.
            La funzione viene specificata completamente definendo $k' = k + |V|W$.
            \begin{equation*}
                f :
                \langle
                G = (V,E^K), c, k
                \rangle
                \to
                \langle
                G = (V,E^K), c', k+|V|W
                \rangle
            \end{equation*}
            E si è mostrato che
            \begin{equation*}
                \langle
                G, c, k
                \rangle
                \in TSP
                \Leftrightarrow
                \langle
                G, c', k+|V|W
                \rangle
                \in T\_TSP
            \end{equation*}
                Per cui $T\_TSP \in \bnpc$.
    \end{itemize}
\end{proof}

\subsection{Algoritmo di 2-approssimazione per  TRIANGLE\_TSP}
\label{sss:ttsp2approx}
% pag 53-59

\subsubsection{Albero di copertura minimo}
% MST, pag 53
\begin{definition}[Albero di copertura]
    \label{def:alberocopertura}
    Dato un grafo
    $G=(V,E)$
    non orientato, connesso, un albero di copertura è un suo sottografo 
    $T_G=(V,E_G)$
    con $|E_G|=|V|-1$ e $T_G$ connesso.
\end{definition}

\begin{definition}[Albero di copertura minimo]
    \label{def:alberocoperturaminimo}
    Sotto la funzione di costo
    \begin{equation*}
        c\left( T_G \right) = 
        \sum_{e \in E_T}^{} c(e)
    \end{equation*}
    L'albero di copertura minimo (\emph{Minimum Spanning Tree}) è l'albero di copertura di $G$ di costo minimo.
\end{definition}

% Prim, pag 53
L'albero di copertura minimo si trova in tempo polinomiale, per esempio usando l'algoritmo di $Prim$ \cite{Prim57}, con complessità $O \left( |E| \log |V| \right)$, quasi lineare nel numero di archi.
\begin{enumerate}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item si seleziona un nodo a caso
    \item si trovano gli archi uscenti
    \item si seleziona l'arco di peso minore
    \item si itera dal punto 2 fino a toccare tutti i nodi
\end{enumerate}

\subsubsection{(Full) Preorder Walk}
% Preorder, pag 54
La visita in preordine dell'albero è chiaramente lineare nel numero di nodi
\begin{algorithm}[H]
\caption{Preorder Walk}\label{alg:preorderwalk}
\begin{algorithmic}[1]
    \Procedure{Preorder\_Visit}{$T,r$}
        \State \Call{Print}{$r$}
        \If{not (leaf (r))}
        \Comment{il nodo corrente ha figli}
        \ForAll{$v \in r.children$}
        % \Comment{RECURSE}
            \State \Call{Preorder\_Visit}{$T,v$}
        \EndFor
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}

% Full Preorder, pag 54
La visita in preordine completa (\emph{Full Preorder Visit}) si effettua stampando di nuovo il nodo quando si è conclusa l'esplorazione del sottoalbero corrispondente.
\begin{algorithm}[H]
\caption{Full Preorder Walk}\label{alg:fullpreorderwalk}
\begin{algorithmic}[1]
    \Procedure{Full\_Preorder\_Walk}{$T,r$}
        \State \Call{Print}{$r$}
        \If{not (leaf (r))}
        \ForAll{$v \in r.children$}
            \State \Call{Full\_Preorder\_Walk}{$T,v$}
            \State \Call{Print}{$r$}
        \EndFor
        \EndIf
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Se si considera l'albero in figura \ref{fig:exalbero}, la visita in preordine risulta 
$ \pi = \langle a,b,c,d,e \rangle$,
mentre la visita in preordine completa è
$ \pi = \langle a,b,c,b,d,b,a,e,a \rangle$.

\begin{figure}[h]
    \centering
    \caption{Esempio di visita in preordine completa}
    \label{fig:expreordervisit}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Albero d'esempio}
        \label{fig:exalbero}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
            ]
            \foreach \pos/\name in {
            {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/c,b/d,a/b,a/e}
                \path[edge] (\source) -- (\dest);
        \end{tikzpicture}
    \end{subfigure}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{$FPW$}
        \label{fig:fullpreordervisit}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                arrow edge/.style = {draw,thick,->},
                bent right/.style = {bend right=20},
                bent left/.style = {bend left=20},
            ]
            \foreach \pos/\name in {
            {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/c,b/d,a/b,a/e}
                \path[arrow edge] (\source) edge [bent right] (\dest);
            \foreach \source/ \dest in {
                    c/b,d/b,b/a,e/a}
                \path[arrow edge] (\source) edge [bent right] (\dest);
        \end{tikzpicture}
    \end{subfigure}
\end{figure}

La visita in preordine completa forma un ciclo (non semplice) all'interno dell'albero (figura \ref{fig:fullpreordervisit}).

\begin{proposizione}
    \label{prop:costofpw}
    Il costo della $FPW$ è due volte il costo del $MST$
    \begin{equation*}
        c(FPW) = 2 \, c(MST)
    \end{equation*}
    \begin{proof}
        La $FPW$ utilizza ogni arco del $MST$ esattamente due volte. La prima volta mentre sta scendendo ricorsivamente l'albero, seleziona $\left\{ r,v \right\}$, e la seconda mentre risale, selezionando $\left\{ v,r \right\}$, come mostrato in figura \ref{fig:fpwcost}.
    \end{proof}
\end{proposizione}
\begin{figure}[ht]
    \centering
    \caption{Costo $FPW$}
    \label{fig:fpwcost}
    \begin{tikzpicture} [
            scale=1,
            vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
            edge/.style = {draw,thick,-},
            arrow edge/.style = {draw,thick,->},
            bent right/.style = {bend right=20},
            bent left/.style = {bend left=20},
        ]
        \foreach \pos/\name in {
                {(1,1.5)/v}, {(2,3)/r}}
            \node[vertex] (\name) at \pos {$\name$};
        \foreach \source/ \dest in {
                r/v,v/r}
            \path[arrow edge] (\source) edge [bent right] (\dest);
            \node at (1,-0.35) {$FPW(T,v)$};
        \begin{pgfonlayer}{background}
            \draw (1,1.5) -- (0,0) -- (2,0) -- cycle;
        \end{pgfonlayer}
    \end{tikzpicture}
\end{figure}

\subsubsection{Algoritmo di approssimazione}
% Approx TTSP, pag 53.8
\begin{algorithm}[H]
\caption{Approssimatore per Triangle TSP}\label{alg:approxttsp}
\begin{algorithmic}[1]
    \Procedure{Approx\_T\_TSP}{$i$}
        \State $T_G \gets$ \Call{Prim}{$ \langle G, c\rangle$}
        \State $\pi \gets$ \Call{Preorder\_Visit}{$T_G$}
        \State * sia $\pi = \langle v_1, v_2, \cdots, v_{|V|}\rangle $ *
        \State return $\langle \pi, v_1 \rangle $
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Se si considera di nuovo l'albero in figura \ref{fig:exalbero}, l'algoritmo ritorna
$ \pi = \langle a,b,c,d,e,a \rangle$.

\begin{figure}[H]
    \centering
    \caption{Risultato dell'algoritmo}
    \label{fig:approxttsp2}
    \begin{tikzpicture} [
            scale=1,
            vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
            edge/.style = {draw,thick,-},
        ]
        \foreach \pos/\name in {
                {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
            \node[vertex] (\name) at \pos {$\name$};
        \foreach \source/ \dest in {
                b/c,c/d,a/b,a/e,d/e}
            \path[edge] (\source) edge (\dest);
    \end{tikzpicture}
\end{figure}

% Fattore approssimazione approx TTSP, pag 55
Riguardo al fattore di approssimazione di questo algoritmo, vanno trovati \emph{bound} per
\begin{equation*}
    \rho =
    \frac{
    |H'|
    }{
    |H^*|
    }
    =
    \frac{
        \text{costo soluzione ammissibile ritornata}
    }{
        \text{costo soluzione ottima}
    }
\end{equation*}
Dove si indica $H'$ la soluzione trovata al problema $T\_TSP$, $H^*$ la soluzione ottima al problema, $T^*$ il \emph{minimum spanning tree} del grafo.

\begin{itemize}
    \item Per $|H'|$: il ciclo ritornato è una sotto sequenza della \emph{full preorder walk}, dove vengono selezionati i nodi la prima volta che vengono visti (ed aggiunta la radice).
        \begin{equation*}
            H' = \langle a,b,c,d,e,a \rangle
            \quad
            FPW = \langle \underline{a},\underline{b},\underline{c},b,\underline{d},b,a,\underline{e},a \rangle
        \end{equation*}
        Per la proprietà dello \emph{shortcutting}, vale $
        c(H') \leq c(FPW)
        $
        e dalla proposizione \ref{prop:costofpw} si conosce $
        c(FPW) = 2 \, c(MST)
        = 2 \, c(T^*)
        $.
        Componendo si ottiene 
        \begin{equation*}
            c(H')
            \leq 2 \, c(T^*)
        \end{equation*}
    \item Per $|H^*|$:
        Se dal ciclo ottimo
        $H^*$
        si rimuove un arco $e$ qualsiasi, si ottiene uno \emph{spanning tree}
        generico
        % (di costo non necessariamente minimo)
        , che è di sicuro non di costo inferiore al migliore albero di copertura trovato:
        \begin{equation*}
            c(H^*) \geq 
            c(H^* - \{ e \} ) =
            c(ST) \geq 
            c(T^*)
        \end{equation*}
\end{itemize}
Componendo i risultati
\begin{equation*}
    \rho =
    \frac{
        |H'|
    }{
        |H^*|
    }
    \leq
    \frac{
        2 \, c(T^*)
    }{
        |H^*|
    }
    \leq
    \frac{
        2 \, c(T^*)
    }{
        c(T^*)
    }
    = 2
\end{equation*}

\subsection{Algoritmo di Christofides}
% pag 56

\subsubsection{Multigrafi e circuiti Euleriani}

% multigrafo, pag 56.2
\begin{definition}[Multigrafo]
    \label{def:multigrafo}
    Un multigrafo $G=(V,E)$ non orientato è costruito su un insieme di nodi e un multiinsieme di archi.
    Ogni arco $e \in E$ può essere presente in multiple copie:
    \begin{equation*}
        \text{molteplicità:}
        \quad
        m(e) = \text{\# copie di $e$}
    \end{equation*}
    Il grado di un nodo deve tenere conto di tutte le copie degli archi incidenti:
    \begin{equation*}
        deg(v) = \sum_{e : v \in e}^{} m(e)
    \end{equation*}
    Cammini e cicli sono definiti come per un grafo.
\end{definition}

% circuito Euleriano, pag 56.5
\begin{definition}[Circuito Euleriano]
    \label{def:circuitoeuleriano}
    Un circuito Euleriano è un ciclo \emph{non semplice} che tocca tutti gli archi del grafo.
    Un (multi)grafo è Euleriano se ammette un circuito Euleriano.
\end{definition}

\begin{definition}[Grafo Euleriano]
    \label{def:grafoeuleriano}
    Un (multi)grafo è Euleriano se ammette un circuito Euleriano.
\end{definition}

% teorema di eulero, pag 56.7
\begin{theorem}
    \label{teo:grafoeuleriano}
    Un multigrafo connesso è Euleriano se e solo se tutti i vertici del grafo hanno grado pari.
\end{theorem}

% prop nodi dispari in numero pari, pag 57.5
\begin{proposizione}
    Dato un (multi)grafo non orientato, il numeri di vertici di grado dispari è sempre pari.
    \begin{proof}
        Ogni arco incide su due vertici, per cui
        \begin{equation*}
            2 |E| 
            =
            \sum_{v \in V}^{} deg(v)
            = 
            \sum_{v \in V_{EVEN}}^{} deg(v)
            +
            \sum_{v \in V_{ODD}}^{} deg(v)
        \end{equation*}
        La somma dei due termini deve essere pari,
        % Dei due termini della somma,
        il primo è pari perché somma di contributi pari,
        quindi anche il secondo deve essere pari.
        Perché una somma di contributi dispari sia pari, deve essere un numero pari di contributi.
    \end{proof}
\end{proposizione}

% def matching perfetto, pag 57.3
\begin{definition}[Matching perfetto]
    \label{def:matchingperfetto}
    Dato un grafo $G$ con $|V| = 2k$ un matching perfetto è un insieme di archi $M \subseteq E$ tale che
    \begin{itemize}
        \item $\forall e_1, e_2 \in M : e_1 \cap e_2 = \emptyset$
        \item $\forall v \in V, \exists e \in M : v \in e$
    \end{itemize}
\end{definition}
Il grafo deve avere un numero pari di nodi perché ogni arco ne copre due.
\\
Se $G$ è pesato esiste un matching perfetto di costo minimo, che può essere individuato con l'algoritmo di \emph{Gabow} in tempo $
O(|E||V|)
$, che in caso di grafo denso ($
|E| = \Theta (|V|^2)
$) diventa $
O(|V|^3)
$.

\subsubsection{Christofides}
% pag 56.9, 57.1

Christofides nel 1976 \cite{Chr76} ha trovato un algoritmo di 3/2-approssimazione per TRIANGLE\_TSP.

L'idea è di partire da un $MST$ ed aggiungere archi in modo da rendere pari ogni nodo, e poi semplificare il cammino Euleriano trovato usando lo \emph{shortcutting}, come mostrato in figura \ref{fig:christofides}.

\begin{figure}[H]
    \centering
    \caption{Algoritmo di Christofides}
    \label{fig:christofides}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{$T^*$ con i nodi dispari evidenziati}
        \label{fig:christmst}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                selected vertex/.style = {vertex, fill=blue!24},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/c,b/d,a/b,a/e}
                \path[edge] (\source) edge (\dest);
            \foreach \vertex in {b,c,d,e}
                \path node[selected vertex] at (\vertex) {$\vertex$};
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Sottografo (completo) indotto dai nodi dispari, e matching perfetto di costo minimo evidenziato}
        \label{fig:christmatching}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                selected edge/.style = {draw,line width=5pt,-,blue!24},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/c,b/d,b/e,c/e,c/d,e/d}
                \path[edge] (\source) edge (\dest);
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {b/c,d/e}
                    \path[selected edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
    \\[2pt]
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Ciclo Euleriano
            $ \langle c,b,d,e,a,b,c \rangle$
        }
        \label{fig:christeulertour}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                bent right/.style = {bend right=20},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/d,a/b,a/e,d/e}
                \path[edge] (\source) edge (\dest);
            \foreach \source/ \dest in {
                    c/b,b/c}
                \path[edge] (\source) edge [bent right] (\dest);
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Dopo lo \emph{shortcutting}:
            $ \langle c,b,d,e,a,\cancel{b},c \rangle$
        }
        \label{fig:christresult}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                bent right/.style = {bend right=30},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/d,a/e,d/e,b/c}
                \path[edge] (\source) edge (\dest);
            \foreach \source/ \dest in {
                    a/c}
                \path[edge] (\source) edge [bent right] (\dest);
        \end{tikzpicture}
    \end{subfigure}
\end{figure}

% Fattore approssimazione, pag 58-59.5

Il grafo ottenuto (prima dello \emph{shortcutting}), è
\begin{equation*}
    G' = 
    (
        V, \,
        E_{T^*}
        \cup
        M^*
    )
\end{equation*}
È già stato mostrato nella sezione \ref{sss:ttsp2approx} che
$ c(T^*) \leq c(H^*) $.
Resta da dimostrare che
% \begin{equation*}
$
    c( M^*)
    \leq
    % \frac{c\left( H^* \right)}{2}
    c\left( H^* \right) / 2
% \end{equation*}
$.
Si supponga di conoscere $H^*$ e di aver calcolato $T^*$. Alcuni nodi in $T^*$ saranno di grado pari \emph{in} $T^*$
(fig \ref{fig:christeven}).
Usando lo \emph{shortcutting}, si rimuovono questi nodi da $
H^*
$
(fig \ref{fig:christremoved}), ottenendo $
H^{*'}
$, con chiaramente $
c( H^{*'})
\leq
c(H^* )
$.
I nodi dispari in $T^*$ erano in numero pari, quindi $
H^{*'}
$ ha un numero pari di vertici.
Colorando in modo alternato gli archi del ciclo
(fig \ref{fig:christredblue}),
gli archi di ciascun colore sono un matching perfetto (non ottimo): $
H^{*'}
= M_B \cup M_R
$, il cui costo è pari al costo del ciclo $
c( H^{*'}) = c(M_B) + c(M_R)
$. Combinando i due vincoli sui costi si ottiene
\begin{equation*}
    c(M_B) + c(M_R)
    \leq
    c(H^* )
\end{equation*}
Se si indica il matching di costo inferiore con $M_X$, deve valere
\begin{equation*}
    c(M_X)
    \leq
    \frac{
    c(H^* )
    }{2}
\end{equation*}
($x+y\leq z \to \min \left\{ x,y \right\} \leq z/2$).
Il matching perfetto 
costruito sui nodi dispari di $T^*$
dall'algoritmo di Christofides è quello ottimo $M^*$, di costo $
    c(M^*)
    \leq
    c(M_X)
$. Combinando,
\begin{equation*}
    c( M^*)
    \leq
    \frac{c\left( H^* \right)}{2}
\end{equation*}
In conclusione
\begin{equation*}
    c (H') \leq
    c ( E_{T^*} \cup M^*) \leq 
    c\left( H^* \right)
    +
    \frac{c\left( H^* \right)}{2}
    =
    \frac{3}{2} \,
    c\left( H^* \right)
\end{equation*}

\begin{figure}[H]
    \centering
    \caption{Costo del matching perfetto ottimo}
    \label{fig:christofidescost}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{$T^*$ con i nodi pari evidenziati}
        \label{fig:christeven}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                selected vertex/.style = {vertex, fill=red!24},
                edge/.style = {draw,thick,-},
            ]
            \foreach \pos/\name in {
                    {(0,0)/c}, {(2,0)/d}, {(1,1.5)/b}, {(3,1.5)/e}, {(2,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/c,b/d,a/b,a/e}
                \path[edge] (\source) edge (\dest);
            \foreach \vertex in {a}
                \path node[selected vertex] at (\vertex) {$\vertex$};
        \end{tikzpicture}
    \end{subfigure}
    \quad
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Tour ottimo $H^*$}
        \label{fig:christoptimalh}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
            ]
            \foreach \pos/\name in {
            {(0,1.8)/c}, {(2.5,0)/d}, {(0.5,0)/b}, {(3,1.8)/e}, {(1.5,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
            b/d,a/e,d/e,b/c,a/c}
                \path[edge] (\source) edge (\dest);
        \end{tikzpicture}
    \end{subfigure}
    \\[2pt]
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Tour ridotto $H^{*'}$}
        \label{fig:christremoved}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                ignored edge/.style = {draw,line width=5pt,-,red!24},
                selected edge/.style = {draw,line width=5pt,-,blue!24},
            ]
            \foreach \pos/\name in {
            {(0,1.8)/c}, {(2.5,0)/d}, {(0.5,0)/b}, {(3,1.8)/e}, {(1.5,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/d,c/e,d/e,b/c}
                \path[edge] (\source) edge (\dest);
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {a/c,a/e}
                    \path[ignored edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {e/c}
                    \path[selected edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \caption{Matching $M_R$ e $M_B$}
        \label{fig:christredblue}
        \begin{tikzpicture} [
                scale=1,
                vertex/.style={circle,fill=black!25,minimum size=20pt,inner sep=0pt},
                edge/.style = {draw,thick,-},
                ignored edge/.style = {draw,line width=5pt,-,red!24},
                selected edge/.style = {draw,line width=5pt,-,blue!24},
            ]
            \foreach \pos/\name in {
            {(0,1.8)/c}, {(2.5,0)/d}, {(0.5,0)/b}, {(3,1.8)/e}, {(1.5,3)/a}}
                \node[vertex] (\name) at \pos {$\name$};
            \foreach \source/ \dest in {
                    b/d,c/e,d/e,b/c}
                \path[edge] (\source) edge (\dest);
            \begin{pgfonlayer}{background}
                \foreach \source / \dest in {b/c,d/e}
                    \path[ignored edge] (\source.center) -- (\dest.center);
                \foreach \source / \dest in {e/c,b/d}
                    \path[selected edge] (\source.center) -- (\dest.center);
            \end{pgfonlayer}
        \end{tikzpicture}
    \end{subfigure}
\end{figure}


\subsection{Altri risultati sull'approssimazione di TSP}

Arora ha mostrato nel 1998 \cite{Arora:1998:PTA:290179.290180} che $EUCLIDEAN\_TSP$ (in cui la distanza per esempio è la norma 2) ammette un algoritmo PTAS,
per cui
$
\forall \varepsilon,
\rho \leq 1+\varepsilon
$
e la complessità è $
O \left( n^{1/\varepsilon} \right)
$.

Papadimitriou e Vempala hanno mostrato nel 2006 \cite{Papadimitriou:2006:s00493-006-0008-z} che il $TRIANGLE\_TSP$ con archi simmetrici non può essere approssimato con un fattore migliore di 220/219.

\section{Set Cover}

\subsubsection{Definizione}

Dato un insieme $X$ e
una famiglia di sottoinsiemi
$
\bff \subseteq \left\{ S : S \subseteq X \right\}
$,
un \emph{set cover } $C$ di $X$ è un insieme di sottoinsiemi $S$ di $X$, tali per cui ogni elemento di $X$ è in almeno un sottoinsieme selezionato
(chiaramente ogni elemento $x \in X$ deve appartenere ad almeno un sottoinsieme $S \in \bff$).
\begin{equation*}
    C \subseteq \bff : 
    \forall x \in X, \exists S \in C :
    x \in S
\end{equation*}
Si cerca il \emph{set cover} di taglia minima $C^*$.

Il problema viene posto come problema decisionale:
\begin{align*}
    SC: & \\
    \texttt{istanza:} \quad & \langle X, \bff, k \rangle \\
    \text{dove} \quad &
    \bff \subseteq \left\{ S : S \subseteq X \right\} = \mathcal{B} \left( X \right)
    \\
    & \forall  x \in X : \exists S \in \bff : x \in S
    \\
    \texttt{domanda:} \quad &
    \exists \, C \subseteq \bff \text{ con } |C| \leq k
\end{align*}

Il problema è simile al \emph{vertex cover}, ma molto più generale, si può quindi ridurre da $VC$ a $SC$.
\begin{proof}
    $VC \lp SC$:
    si esibisce una trasformazione da un problema di $VC$ in uno di $SC$, rappresentando un grafo come insieme $x$ e $\bff$.
    \begin{align*}
        f
        &
        : 
        \langle G = \left( V,E \right), k \rangle 
        \in VC
        \to
        \langle X, \bff, k \rangle 
        \in SC
        \\*
        X &= E
        \\*
        \bff &= 
        \left\{ N_v = \left\{ 
                e \in E : v \in e
            \right\}
            \forall v \in V
        \right\}
    \end{align*}
    Dove $\bff$ è definito come la capacità di copertura di ogni vertice.
    Se si conosce un $VC$, è un insieme di nodi che copre ogni arco. Si selezionano i corrispettivi insiemi $N_v$, a ciascuno dei quali appartengono tutti gli archi coperti da $v$. In questo modo ogni elemento di $X$ appartiene ad almeno un insieme $N_v$.
    \\
    La trasformazione è lineare nella taglia dell'istanza, ed è da notare che \emph{preserva} l'approssimazione.
\end{proof}

\subsection{Approccio \emph{Greedy}}

L'approccio più intuitivo si rivela estremamente efficace: si seleziona ad ogni iterazione l'insieme $T \in \bff$ che contiene il numero massimo di elementi ancora da coprire.
\begin{algorithm}[H]
\caption{Approssimatore per \emph{Set Cover}}\label{alg:approxsc}
\begin{algorithmic}[1]
    \Procedure{Approx\_Set\_Cover}{$ \langle X, \bff \rangle $}
        \State $U \gets X$
        \Comment{elementi ancora da coprire}
        \State $C \gets \emptyset$
        \While{$U \ne \emptyset$}
            \State $S \gets \argmax \left\{ | T \cap U | : T \in \bff \right\}$
            \State $U \gets U - S$
            \Comment{si rimuovono da $U$ gli elementi in $U \cap S$}
            \State $C \gets C \cup \left\{ S \right\}$
        \EndWhile
        \State return $C$
        % \Comment{CONQUER}
    \EndProcedure
\end{algorithmic}
\end{algorithm}
L'algoritmo è terminante, perché $\bff$ è una famiglia di copertura, ossia ogni elemento di $X$ appartiene ad almeno uno degli insiemi $S$.
Almeno un elemento del residuo $U$ è quindi nell'insieme $T$ selezionato, e quindi almeno un elemento viene rimosso da $U$ a ciascuna iterazione; $|U|$ decresce monotonicamente, e quando si svuota l'algoritmo termina.

L'algoritmo è corretto, perché termina proprio quando tutti gli elementi sono coperti.

L'algoritmo ha complessità ragionevole. Con un implementazione naive, il numero di iterazioni è pari a $
\min \{ |X|, |\bff| \}
$, perché si copre almeno un elemento per iterazione ($|X|$) e se venissero selezionati tutti i sottoinsiemi l'algoritmo terminerebbe di sicuro, perché la famiglia è di copertura (|$\bff|$).
Il calcolo dell'$\argmax$ costa $|\bff||X|$ al caso peggiore, bisogna verificare se ogni elemento appartiene ad ogni insieme della famiglia.
La complessità risulta quindi $
O \left( 
\min \{ |X|, |\bff| \}
|\bff||X|
\right)
$, ovvero cubica nella taglia dell'istanza.

Con una struttura dati più sofisticata, si ottiene un'implementazione di complessità lineare dell'algoritmo
$
\Theta \left( 
    \sum_{s \in S}^{} |S|
\right)
$.

\subsubsection{Fattore di approssimazione}

La gloriosa analisi di Pucci risulta assai più intuitiva delle paginate di conti del Cormen.

Si definiscono:
% \begin{itemize}
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item $n = |X|$: la taglia di $X$
    \item $U_t$:
        l'insieme di elementi ancora da coprire all'inizio dell'iterazione $t$
        (vale $U_1 = X$)
    \item $S_t$:
        il sottoinsieme selezionato all'iterazione $t$:
        $S \gets \argmax \left\{ | T \cap U_t | : T \in \bff \right\}$
\end{itemize}
Se si eseguono $h$ iterazioni, allora $|C|=h$.
Si studia la dinamica di $|U_t|$, che deve essere veloce, se si vuole coprire con pochi insiemi gli elementi.
L'insieme $X$ è coperto ottimamente dall'insieme ignoto $
C^* = \left\{
    S_1^*,
    S_2^*,
    \cdots,
    S_k^*
\right\}
$, di taglia $|C^*|=k$.
All'iterazione generica $t>0$, ci sono ancora degli elementi scoperti in $U_t$.
Esistono $k$ sottoinsiemi di $\bff$ che coprono $U_t$, e per \emph{pigeon hole}, almeno uno di questi deve essere grande:
\begin{equation*}
    \exists S_j^* :
    | S_j^* \cap U_t | \geq \frac{|U_t|}{k}
\end{equation*}
Se tutti gli insiemi fossero di taglia minore, anche immaginandoli composti di elementi distinti, non coprirebbero $U_t$.
Si è così trovato un rate rispetto allo stato attuale della taglia di $U_t$, che comporta una disequazione di ricorrenza:
\begin{align*}
    | U_{t+1} |
    \leq
    |U_t| - \frac{|U_t|}{k}
    &= \left( 1- \frac{1}{k} \right) |U_t|
    \\
    & \leq
    \left( 1- \frac{1}{k} \right)^2 |U_{t-1}|
    \leq
    \left( 1- \frac{1}{k} \right)^3 |U_{t-2}|
    \leq \ldots
    \\
    \text{(per \emph{unfolding}) } \quad \quad
    % & \leq \ldots
    % \\
    & \leq
    \left( 1- \frac{1}{k} \right)^{i+1} |U_{t-i}|
    % \\
    % \text{per \emph{unfolding}: }
    % & \leq \ldots
\intertext{Il caso base si raggiunge per $i= t -1$}
    | U_{t+1} |
    & \leq \left( 1- \frac{1}{k} \right)^t |U_1|
    \\
    \text{(vale  $|U_1| = |X| = n$) } \quad \quad
    & 
    = n \left( 1- \frac{1}{k} \right)^t
    \intertext{
        Vale
        $1-x \leq e^{-x}$.
        Per $x>0$ la disuguaglianza è stretta.
        $(1-x)^t \leq (e^{-x})^t$
    }
    \text{($x=1/k$) } \quad \quad
    & <
    n e^{-t/k}
    \\
% \intertext{Scegliendo $t = k \log n$}
    \text{(Scegliendo $t = k \log n$) } \quad \quad
    &=
    n e^{-(k \log n)/k}
    = n e^{-n} 
    =  1
% \intertext{Ovvero si è mostrato che:}
    \\
    | U_{t+1} |
    <  1
    &
    \Rightarrow
    | U_{t+1} |
    = 0
\intertext{Per cui la $(t+1)$-esima iterazione non avviene.
L'insieme $C$ ha taglia pari al numero di iterazioni, per cui
}
    |C| &= t \leq k \log n
    = |C^*| \log n
    \\
    \rho (n) &= 
    \frac{|C|}{|C^*|} \leq \log n
\end{align*}

\subsubsection{Fattore di approssimazione più stretto}

Il fattore di approssimazione ricavato è relativo al caso peggiore.
In alcune istanze particolari, però, si riesce a ricavare un \emph{bound} molto più stretto, addirittura costante, legato ai numeri armonici.

\begin{definition}[Numero armonico]
    \label{def:numeroarmonico}
    Un numero armonico è dato dalla serie troncata degli inversi dei naturali
    \begin{equation*}
        H_k = \sum_{i=1}^{k} \frac{1}{i}
    \end{equation*}
    I valori dei numeri armonici sono legati strettamente ai logaritmi, infatti la \rfig{fig:numeriarmonici} mostra come si possa limitare il valore
    \begin{equation*}
        1 + \int_{1}^{k} \frac{1}{x} dx
        \leq
        H_k
        \leq
        \int_{1}^{k+1} \frac{1}{x} dx
    \end{equation*}
    che integrando risulta
    \begin{equation*}
        \log \left( k+1 \right)
        \leq
        H_k
        \leq
        1 + \log \left( k \right)
    \end{equation*}
    e che si approssima come
    \begin{equation*}
        H_k = \log k + O\left( 1 \right)
    \end{equation*}
\end{definition}

\begin{figure}[htp]
    \centering
    \begin{tikzpicture}
        \begin{axis}
            \addplot[
                color=red,
                domain=0.5:5,
            ]{1/x};
            \addlegendentry{$1/x$} 
            \addplot[
                color=green,
                ]
                coordinates {
                (1,1)
                (2,1)
                (2,1/2)
                (3,1/2)
                (3,1/3)
                (4,1/3)
                (4,1/4)
                (5,1/4)
                };
            \addlegendentry{Limite superiore} 
            \addplot[
                color=blue,
                ]
                coordinates {
                (0,1)
                (1,1)
                (1,1/2)
                (2,1/2)
                (2,1/3)
                (3,1/3)
                (3,1/4)
                };
            \addlegendentry{Limite inferiore} 
        \end{axis}
    \end{tikzpicture}
    \caption{Approssimazione dei numeri armonici}
    \label{fig:numeriarmonici}
\end{figure}

Il fattore che si trova è
\begin{equation*}
    \rho \leq H_{\bar{k}}
    \approx \log (\bar{k})
    \quad
    \text{con}
    \quad
    \bar{k} = \max \left\{ |S| : S \in \bff \right\}
\end{equation*}
Se l'insieme più grande nella famiglia è di taglia paragonabile all'intero insieme $X$, il fattore resta logaritmico nella taglia dell'istanza. Assai più interessante il caso in cui $\bff$ è composta da molti insiemi piccoli: se la taglia è costante, anche $\rho$ lo è.

Per esempio, in un caso particolare di \emph{vertex cover}, si può usare la soluzione approssimata per \emph{set cover} ottenendo una soluzione di qualità migliore.
Si modifica l'istanza di $VC$, aggiungendo il vincolo $
deg(v) \leq 3, \forall v \in V
$.
Questo problema resta NP-completo, in maniera simile a $3-CNF-SAT$ è il caso estremo di difficoltà.
Si usa la trasformazione vista per la riduzione, che produce la famiglia
$
        \bff = 
        \left\{ N_v = \left\{ 
                e \in E : v \in e
            \right\}
            \forall v \in V
        \right\}
$
con chiaramente $|N_v| \leq 3$.
Il fattore di approssimazione risulta allora
\begin{equation*}
    \rho = H_3 =
    \frac{1}{1} +
    \frac{1}{2} +
    \frac{1}{3} = 
    \frac{11}{6} < 2
\end{equation*}
migliore di quello visto per la soluzione approssimata di $VC$.

\begin{proof}
    Si esegue un'analisi ammortizzata, assegnando un peso agli elementi coperti: se ad un'iterazione si coprono parecchi elementi, questi avranno costo basso, mentre in un'iterazione ``sprecata'', che copre pochi elementi, sarà assegnato un costo maggiore.
    Si associa quindi peso unitario ad ogni iterazione, che viene suddiviso uniformemente sugli elementi coperti a quell'iterazione.
    \\
    La notazione usata è la stessa, all'iterazione $t$ mancano ancora da coprire gli elementi in $U_t$, e viene selezionato l'insieme $S_t$.
    \\
    Agli elementi coperti per la prima volta si associa un costo nella maniera seguente
    \begin{equation*}
        \forall x \in
        U_t \cap S_t
        \quad
        \leadsto
        \quad
        c_x =
        \frac{1}{
            | U_t \cap S_t |
        }
    \end{equation*}
    Si calcola per primo il costo della soluzione ammissibile ricavata
    \begin{align*}
        |C|
        &=
        \sum_{t=1}^{|C|} 1
        \\
        &=
        \sum_{t=1}^{|C|} 
        \sum_{x \in 
            U_t \cap S_t
        }
        \frac{1}{
            | U_t \cap S_t |
        }
        \\
        &=
        \sum_{t=1}^{|C|} 
        \sum_{x \in 
            U_t \cap S_t
        }
        c_x
        \intertext{gli insiemi $U_t \cap S_t$ sono disgiunti ad ogni iterazione, si coprono solo nuovi elementi, quindi la somma si semplifica alla somma di tutti i pesi possibili}
        &= 
        \sum_{x \in X}^{} c_x
    \end{align*}
    La relazione tra il \emph{set cover} ottimo e la pesatura si ricava utilizzando il lemma seguente, la cui dimostrazione è molto articolata.
    \begin{lemma}
        \label{lem:armonico}
        Vale, $\forall S \in \bff$
        \begin{equation*}
            \sum_{x \in S}^{} c_x \leq
            H_{|S|}
        \end{equation*}
    \end{lemma}
    Si prosegue quindi dal risultato raggiunto
    \begin{align*}
        |C|
        &= 
        \sum_{x \in X}^{} c_x
        \intertext{il \emph{set cover} ottimo prende ogni nodo \emph{almeno} una volta}
        & \leq 
        \sum_{S \in C^*}^{} \left( 
            \,
            \sum_{x \in S}^{} c_x
        \right)
        \intertext{applicando il lemma \ref{lem:armonico}}
        & \leq 
        \sum_{S \in C^*} H_{|S|}
        \intertext{selezionando il termine massimo e sommandolo $|C^*|$ volte, risulta}
        & \leq 
        |C^*| \,
        H_{
            \max \left\{ |S| : S \in \bff \right\}
        }
        \intertext{ovvero}
        \rho = \frac{|C|}{|C^*|}
        & \leq \,
        H_{
            \max \left\{ |S| : S \in \bff \right\}
        }
    \end{align*}
\end{proof}



\section{FPTAS per Subset Sum}
\label{sec:ss_fptas}

\subsubsection{Definizione del problema di ottimizzazione e decisionale}

Il problema di \emph{subset sum} si pone come problema di ottimizzazione di massimo
\begin{align*}
    MAX\_SS: & \\
    \texttt{istanza:} \quad &
    \langle S,t \rangle
    \\
    \text{dove} \quad & S \subseteq \mathbb{N} - \left\{ 0 \right\} \text{ finito} \\
    & t \in \mathbb{N} \\
    \texttt{costo:} \quad &
    c(S) = \sum_{s \in S'} s \leq t \\
    \texttt{soluzioni ammissibili:} \quad &
    \bs \left( i \right)
    =
    \left\{ 
        S' \subseteq S : c(S) \leq t
    \right\}
    % \\
    % \texttt{soluzione ottima:} \quad &
    % s^* = \argmax 
    % \left\{ 
        % c(s) : s \in
        % \bs \left( i \right)
    % \right\}
\end{align*}
E nella sua versione decisionale
\begin{align*}
    D\_SS: & \\
    \texttt{istanza:} \quad & \langle S,t,k \rangle \\
    \texttt{domanda:} \quad &
    \exists
    S' \subseteq S
    \text{ ammissibile
    % } c(S') \leq t
    % \\
    % & \text{
    tale che } c(S') \geq k
\end{align*}

\subsection{FPTAS}

Data una certa istanza $
\langle S,t \rangle
$, con $|S|=n$, fissato $\varepsilon$, si vuole trovare un algoritmo $
A\_SS (
\langle S,t \rangle
, \varepsilon )
$ che ritorni una soluzione ammissibile $
S' \subseteq S
$ con una certa qualità garantita
\begin{equation*}
    \forall \varepsilon,
    \quad
    \rho = \frac{
        c(S^*)
    }{
        c(S)
    } \leq 1 + \varepsilon
\end{equation*}
L'algoritmo deve essere polinomiale nella taglia dell'istanza ($
poly (
\langle S,t \rangle
)
$, ovvero $ poly(n) $ e $poly( \log t)$), e polinomiale in $1/\varepsilon$.

Si cerca qualcosa di molto uniforme, per $\varepsilon \to 0$ deve diventare esatto. Si parte da uno schema enumerativo (esponenziale), e lo si modula in funzione di $\varepsilon$ in modo da enumerare soltanto un sottoinsieme delle soluzioni, ottenendo un algoritmo di complessità scalabile in $\varepsilon$.

\subsubsection{Definizioni prerequisite}

Nel corso della dimostrazione saranno utilizzati strutture dati, operatori e algoritmi specifici.

Sia $
S = \left\{ x_1, \cdots, x_n \right\}
$, un suo prefisso è $
S_i = \left\{ x_1, \cdots, x_i \right\}
$.
Vale $ S_0 = \emptyset $.

\begin{definition}
    \label{def:listasenzaduplicati}
    $L_i$ è una lista ordinata, senza duplicati di tutti i costi ammissibili di sottoinsiemi di $S_i$.
    Il suo primo elemento è sempre 0, assumendo che l'insieme vuoto sommi a 0: $
    L_0 = \langle 0 \rangle
    \leftrightarrow
    S_0 = \emptyset
    $.
    Gli elementi della lista sono al massimo $2^i$.
    Per esempio:
    \begin{align*}
        S &= 
        \left\{ x_1 = 3, x_2 = 4, x_3 = 7 \right\}, t = 12
        \\
        L_0 &= \langle 0 \rangle
        \\
        L_1 &= \langle 0, 3 \rangle
        \\
        L_2 &= \langle 0, 3, 4, 7 \rangle
        \\
        L_3 &= \langle 0, 3, 4, 7, 10, 11 \rangle
    \end{align*}
\end{definition}
\begin{definition}
    \label{def:operatorept}
    Si definisce l'operatore $
    \pt \left( L_i, x \right)
    $, che somma a una lista un costo, mantenendo solo i costi ammissibili.
    Lo si scrive in maniera infissa $
    L_i \pt x
    $.
    \begin{algorithm}[H]
    \caption{Somma ammissibile}\label{alg:operatorept}
    \begin{algorithmic}[1]
        \Procedure{$\pt$}{$L, x$}
            \State * $L = \langle y_0, \cdots, y_{k-1} \rangle $ *
            \State $k \gets L.len$
            \State $i \gets 0$
            \State $L' \gets \langle \rangle $
            \While{ $
                \left( y_i + x \leq t \right)
                \wedge
                \left( i < k \right)
            $ }
                \State $L' \gets \langle L', y_i + x \rangle $
                \State $i \gets i + 1$
            \EndWhile
            \State return $L'$
        \EndProcedure
    \end{algorithmic}
    \end{algorithm}
    Il \emph{while} gira finché il nuovo costo è inferiore al target, e ci sono elementi nella lista.
\end{definition}
\begin{definition}
    La procedura $MERGE\_SD$ esegue il \emph{merge} senza duplicati tra due liste, in tempo lineare se le liste sono ordinate.
    \label{def:mergesd}
\end{definition}

\subsection{Solutore esatto esponenziale}

\begin{algorithm}[H]
\caption{Solutore esponenziale per subset sum}\label{alg:ss_exp}
\begin{algorithmic}[1]
    \Procedure{EXP\_SS}{$ \langle S,t \rangle $}
        \State $n \gets |S|$
        \State * $ S = \left\{ x_1, \cdots, x_n \right\} $ *
        \State $ L_0 \gets \langle 0 \rangle $
        \For{$i \gets 1 $ to $ n $ }
            \State $L_i \gets \Call{Merge\_SD}{
                        L_{i-1},
                        L_{i-1} \pt x_i
                    }$
        \EndFor
        \State return $\max (L_n)$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
L'algoritmo genera tutte le soluzioni ammissibili: all'iterazione $i$, unisce tutti i sottoinsiemi che non contengono $x_i$, i cui costi sono già in $L_{i-1}$, e i sottoinsiemi che contengono $x_i$, mantenendo solo quelli ammissibili con la somma $ L_{i-1} \pt x_i $.

La lunghezza delle liste al massimo raddoppia ad ogni iterazione, $|L_i| \leq 2|L_{i-1}|$ per $i\geq 1$, e $|L_0|=1$, per cui $|L_i| = O \left( 2^i \right)$.

Le operazioni eseguite sono lineari nella lunghezza delle liste, per cui la complessità risulta
\begin{align*}
    T_{EXP\_SS} 
    =
    \sum_{i=1}^{n}
    \Theta 
    \left( 
        |L_i|
    \right)
    = 
    \Theta 
    \left( 
        \sum_{i=1}^{n}
        2^i
    \right)
    = O \left( 2^n \right)
    = O \left( 2^{|S|} \right)
\end{align*}

\subsection{Soluzione approssimata}

La versione approssimata dell'algoritmo si ottiene considerando non l'intera lista di costi ammissibili, ma solo un suo campionamento. Si eliminano costi ``simili'', che possono essere approssimati l'uno dall'altro. Si mantengono rappresentazioni per difetto.
% TODO chiarisci perché per difetto pag 67
\begin{definition}
    \label{def:deltadiradamento}
    Un $\delta$-diradamento, detto anche $\delta$-trim o trim di una lista $L$, è una sua sottolista $L'$ tale che
    \begin{equation*}
        \forall y \in L, \,
        \exists z \in L' :
        \frac{y}{1+\delta} \leq z \leq y
    \end{equation*}
    che si riscrive come $
        z \leq y \leq z \left( 1+\delta \right)
    $: è un approssimazione per difetto, ma ogni $y$ non è troppo diverso da $z$.
    Il minimo della lista è sempre apprissimato da sé stesso.
    Per esempio $
        L = \langle
            2, 10, 11, 12, 20, 21
            \rangle, \delta = 1/10 $ è approssimata da $
        L' = \langle
            2, 10, 12, 20
            \rangle
    $.
\end{definition}
Esiste un algoritmo greedy per ottenere il trimming minimo, di complessità lineare se la lista è ordinata.
\begin{algorithm}[H]
\caption{Estrazione del trimming minimo}\label{alg:trimming_minimo}
\begin{algorithmic}[1]
    \Procedure{Trim}{$L, \delta$}
        \State * $L = \langle y_0, \cdots, y_{k-1} \rangle $ *
        \State $L' \gets \langle y_0 \rangle $
        \State $last \gets y_0$
        \For{$i \gets 1 $ to $ k-1 $ }
            \If{$y_i > last (1-\delta)$}
            \label{alg:trimming_minimo:check}
                \State $L' \gets \langle L', y_i \rangle $
                \State $last \gets y_i$
            \EndIf
        \EndFor
        \State return $L'$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
Se la lista ridotta è $
L' = \langle z_0, \cdots, z_{h} \rangle
$, per $j \geq 1$ vale $
z_{j+1} > z_j \left( 1+\delta \right)
$, perché l'elemento successivo viene selezionato solo se il check alla riga \ref{alg:trimming_minimo:check} passa.
Il rate di approssimazione degli elementi è quindi
\begin{equation*}
    \frac{
        z_{j+1}
    }{
        z_j
    } >  1+\delta 
\end{equation*}
Se si dirada il diradamento $i$ volte, questo rate cresce in maniera esponenziale: $
\left( 1+\delta \right)^i
$.

Si può quindi modificare l'algoritmo ottenendo
\begin{algorithm}[H]
\caption{Solutore approssimato per subset sum}\label{alg:ss_approx}
\begin{algorithmic}[1]
    \Procedure{Approx\_SS}{$ \langle S,t \rangle, \varepsilon$}
        \State $n \gets |S|$
        \State * $ S = \left\{ x_1, \cdots, x_n \right\} $ *
        \State $ L_0 \gets \langle 0 \rangle $
        \For{$i \gets 1 $ to $ n $ }
            \State $L_i \gets \Call{Merge\_SD}{
                        L_{i-1},
                        L_{i-1} \pt x_i
                    }$
            \State $L_i \gets \Call{Trim}{
                        L_{i},
                        % \frac{\varepsilon}{2n}
                        \varepsilon / 2n
                    }$
        \EndFor
        \State return $\max (L_n)$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

L'algoritmo è corretto, perché il costo ritornato è il costo di un sottoinsieme, non vengono creati elementi spuri.

\subsubsection{Fattore di approssimazione}
% pag 69

Siano $
Z^* = c (S^*)
$ il costo ottimo e $
Z' = \max \left( L_n \right)
$ il valore ammissibile restituito.
Si vuole dimostrare che
\begin{equation*}
    \rho = \frac{Z^*}{Z'} \leq 1 + \varepsilon
\end{equation*}

Per ricavare il fattore di approssimazione, occorre dimostrare il seguente lemma:
\begin{lemma}
    \label{lem:ss_costo_approx}
    Sia $y$ un costo ammissibile arbitrario di un sottoinsieme $S' \subseteq S_i$
    \begin{equation*}
        y = \sum_{s \in S'} s \leq t \\
    \end{equation*}
    Allora $y$ è approssimato da un elemento $z \in L_i $ tale che
    \begin{equation*}
        \frac{y}{
            \left( 
            1+
            % \frac{\varepsilon}{2n}
            \varepsilon / 2n
            \right)^i
        } \leq z \leq y
    \end{equation*}
    \begin{proof}
        Si dimostra per induzione su $i$.
        \\
        \emph{Base:}
        Per $i=0,
        L_0 = \langle 0 \rangle 
        $ che approssima sé stesso.
        \\
        \emph{HP induttiva:}
        Per $i>0$
        \begin{equation*}
            \forall S_{i-1} \quad \text{ vale } 
            \quad
            \frac{y}{
                \left( 
                1+
                \varepsilon / 2n
                \right)^{i-1}
            } \leq z \leq y
        \end{equation*}
        \\
        \emph{Tesi:}
        Per un generico $S' \subseteq S_i$, possono verificarsi due casi
        \begin{enumerate}
            \item $x_i \notin S'$: per cui $S' \subseteq S_{i-1}$ e vale l'ipotesi induttiva per $i-1$
                \begin{equation*}
                    \exists z' \in L_{i-1} :
                    \frac{y}{
                        \left( 
                        1+
                        \varepsilon / 2n
                        \right)^{i-1}
                    } \leq z' \leq y
                \end{equation*}
                ma non è detto che $z'$ sia selezionato dal trim, e venga mantenuto in $L_i$
                \begin{itemize}
                    \item $z' \in L_i$: allora c'è un elemento nella lista per cui
                        % $\exists z' \in L_{i} :$
                        \begin{align*}
                            \leq
                            &
                            \frac{y}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)^{i-1}
                            }
                            \leq z' \leq y
                            \\
                            \frac{y}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)^{i}
                            }
                            \leq
                            &
                            \quad
                            \text{\textquotedbl}
                        \end{align*}
                    \item $z' \notin L_i$: $z'$ viene approssimato da $z'' \in L_{i} $
                        \begin{align*}
                            \leq
                            &
                            \frac{z'}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)
                            } \leq z'' \leq z'
                            \\
                            \frac{y}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)^{i-1}
                            }
                            \frac{1}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)
                            }
                            \leq
                            &
                            \quad
                            \text{\textquotedbl}
                            \quad \text{(per l'HP induttiva)}
                            \\
                            \frac{y}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)^{i}
                            }
                            \leq
                            &
                            \quad
                            \text{\textquotedbl}
                        \end{align*}
                \end{itemize}
            \item $x_i \in S'$: ovvero $x_i$ è un elemento nuovo.
                Sia $S'' = S' - \{ x_i \} $
                \begin{equation*}
                    y = c(S') =
                    \sum_{s \in S'} s =
                    \sum_{s \in S''} s + x_i
                    \\
                \end{equation*}
                e $S'' \subseteq S_{i-1}$, per cui vale l'HP induttiva su $S''$
                \begin{equation*}
                    \sum_{s \in S''} s =
                    y - x_i
                    \\
                \end{equation*}
                ed esiste $z' \in L_{i-1}$ per cui
                \begin{align*}
                    \frac{
                    y - x_i
                    }{
                        \left( 
                        1+
                        \varepsilon / 2n
                        \right)^{i-1}
                    }
                    \leq
                    &
                    \,
                    z'
                    \leq
                    y - x_i
                    % \\
                    \intertext{si somma $x_i$ a tutti i membri}
                    \frac{
                    y - x_i
                    }{
                        \left( 
                        1+
                        \varepsilon / 2n
                        \right)^{i-1}
                    }
                    + x_i
                    \leq
                    &
                    \,
                    z'
                    + x_i
                    \leq
                    y
                    % \\
                    \intertext{dividendo $x_i$ per una quantità maggiore di 1 il valore scende}
                    \frac{
                    y - x_i
                    }{
                        \left( 
                        1+
                        \varepsilon / 2n
                        \right)^{i-1}
                    }
                    +
                    \frac{
                    x_i
                    }{
                        \left( 
                        1+
                        \varepsilon / 2n
                        \right)^{i-1}
                    }
                    \leq
                    &
                    \quad
                    \text{\textquotedbl}
                    \\
                    \frac{
                    y
                    }{
                        \left( 
                        1+
                        \varepsilon / 2n
                        \right)^{i-1}
                    }
                    \leq
                    &
                    \quad
                    \text{\textquotedbl}
                \end{align*}
                $ z' \in L_{i-1}
                $ è l'approssimatore usato dall'HP induttiva ma non è detto che \\ $
                z'' = z' + x_i \in L_{i-1} \pt x_i
                $
                \begin{itemize}
                    \item $\bar{z} \in L_i$: come nel caso 1,
                        \begin{equation*}
                            \frac{y}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)^{i}
                            }
                            \leq
                            \frac{y}{
                                \left( 
                                1+
                                \varepsilon / 2n
                                \right)^{i-1}
                            }
                            \leq \bar{z} \leq y
                        \end{equation*}
                    \item $\bar{z} \notin L_i$: esiste $
                        \bar{\bar{z}}
                        \in L_i
                        $ che approssima $\bar{z}$
                        e ci si riporta al caso 1
                        % TODO ma COME?
                \end{itemize}
        \end{enumerate}
    \end{proof}
\end{lemma}
Dal lemma, esiste $z' \in L_n$ che approssima il costo ottimo $z^*$:
\begin{equation*}
    \frac{z^*}{
        \left( 
        1+
        \varepsilon / 2n
        \right)^n
    } \leq z' \leq z^* 
    = c(S^*)
\end{equation*}
Per cui vale
\begin{align*}
    \leq
    &
    \frac{z^*}{
        z'
    }
    =
    \left( 
        1+
        % \varepsilon / 2n
        \frac{\varepsilon}{2n}
    \right)^n
    \\
    \rho \left( n, \varepsilon \right)
    = 
    \frac{z^*}{
        \max \left( L_n \right)
    }
    \leq
    &
    \quad
    \text{\textquotedbl}
    \quad
    \left( \max \left( L_n \right) \geq z' \right)
\end{align*}
La funzione con cui si effettua la maggiorazione su $\rho$ è crescente (ha derivata positiva)
e il suo limite è
\begin{equation*}
    \lim_{n \to + \infty}
    \left( 1+
        \frac{\varepsilon}{2n}
    \right)^n
    = 
    e^{\varepsilon/2}
\end{equation*}
per cui viene limitata superiormente da questa quantità
\begin{align*}
    \rho \left( n, \varepsilon \right)
    &=  
    \left( 1+
        \frac{\varepsilon}{2n}
    \right)^n
    \leq 
    e^{\varepsilon/2}
    % \\
    \intertext{per $x<1$ vale $e^x < 1 + x + x^2$}
    % NOTEVOLE
    % \text{(per $x<1$ vale $e^x < 1 + x + x^2$)}
    % \quad
    &
    \leq 1 +
    \frac{\varepsilon}{2} +
    \left( 
        \frac{\varepsilon}{2}
    \right)^2
    \\
    \left( 
    \left( 
        % \frac{\varepsilon}{2}
        \varepsilon/2
    \right)^2
    \leq 
    % \frac{\varepsilon}{2}
    \varepsilon/2
    \right)
    \quad
    &
    \leq 1 +
    \frac{\varepsilon}{2} +
    \frac{\varepsilon}{2}
    \\
    &
    = 1 + \varepsilon
\end{align*}

\subsubsection{Complessità}
% pag 72

Per quanto riguarda la complessità, ad ogni iterazione il numero di operazioni è legato alla lunghezza della lista, per cui
\begin{align*}
    % T_{A\_SS} &= 
    T_{A\_SS} = 
    O \left( 
        \sum_{i=1}^{n} | L_{i-1} |
    \right)
    % \\
    % &= 
    = 
    O \left( 
        \sum_{i=0}^{n-1} | L_{i} |
    \right)
    % \\
\end{align*}
Sia $
L_i = \langle z_0 = 0, z_1, \cdots,
z_{k_i - 1}
\rangle 
$, allora $
| L_i | = k_i
$.
Si è già visto che 
\begin{equation*}
    \frac{
        z_{j+1}
    }{
        z_j
    }
    >
    % \left(
        1+
        \frac{\varepsilon}{2n}
    % \right)
\end{equation*}
Si può scrivere con una serie telescopica
\begin{align*}
    \frac{
        z_{k_i - 1}
    }{
        z_1
    }
    =
    \frac{
        z_{k_i - 1}
    }{
        z_{k_i - 2}
    }
    \frac{
        z_{k_i - 2}
    }{
        z_{k_i - 3}
    }
    \cdots
    \frac{
        z_2
    }{
        z_1
    }
    >
    \left(
        1+
        \frac{\varepsilon}{2n}
    \right)^{k_i -2}
\end{align*}
Ovvero
\begin{align*}
    z_{k_i - 1}
    >
    \left(
        1+
        \frac{\varepsilon}{2n}
    \right)^{k_i -2}
    z_1
\end{align*}
E ricordando che il costo è ammissibile, $t \geq
z_{k_i - 1}
$, e $z_1 \geq 1$, vale
\begin{align*}
    t >
    % z_{k_i - 1}
    % >
    % \left(
        % 1+
        % \frac{\varepsilon}{2n}
    % \right)^{k_i -2}
    % z_1
    % \geq
    \left(
        1+
        \frac{\varepsilon}{2n}
    \right)^{k_i -2}
\end{align*}
Questa maggiorazione indica che il valore dell'esponente non può essere elevato, e la lunghezza delle liste non può essere elevata. Si cerca quindi una maggiorazione di $k_i$ rispetto a $n,t$ (che sono legati alla taglia dell'istanza). Prendendo il logaritmo dei membri
\begin{align*}
    \left( 
        k_i - 2
    \right)
    \log \left( 
        1+
        \frac{\varepsilon}{2n}
    \right)
    &<
    \log t
    \\
    k_i 
    &<
    \frac{
        \log t
    }{
        \log \left( 
            1+
            \frac{\varepsilon}{2n}
        \right)
    }
    + 2
    \intertext{
        per $|x|<1$
        vale
        $\log \left( 1+x \right) 
        \geq
        % \frac{x}{1+x}
        x/\left( 1+x \right)
        $
    }
    % k_i 
    &<
    \log t
    \frac{
        \left( 
            1+
            % \frac{\varepsilon}{2n}
            \varepsilon/2n
        \right)
    }{
        % \frac{\varepsilon}{2n}
        \varepsilon/2n
    }
    + 2
    \intertext{e $\forall \varepsilon \leq n$ vale 
        $
        1 +
        \varepsilon/2n
        \leq
        3/2
        $
    }
    % k_i 
    &<
    \frac{3}{2}
    \frac{2n}{\varepsilon}
    \log t
    + 2
    \intertext{sia $n$ sia $\log t$ sono sublineari nella taglia dell'istanza}
    k_i &= 
    O \left( 
        \frac{
            | \langle S,t \rangle |^2
        }{
            \varepsilon
        }
    \right)
    \intertext{tornando alla complessità, $k_i = |L_i|$}
    T_{A\_SS}
    \left( 
        | \langle S,t \rangle |
    \right)
    &= 
    O \left( 
        \sum_{i=0}^{n-1} | L_{i} |
    \right)
    =
    O \left( 
        \frac{
            | \langle S,t \rangle |^3
        }{
            \varepsilon
        }
    \right)
\end{align*}
Come desiderato, la complessità è poliomiale nella taglia dell'input e in $1 / \varepsilon$.

\section{Altre tecniche di approssimazione}

Tutto il sapere umano sugli algoritmi di approssimazione è raccolto in \emph{Approximation Algorithms} \cite{Vazirani:2001:AA:500776} di Vijay V. Vazirani.

\subsection{Algoritmi randomizzati}

\subsubsection{Paradigma}

\begin{definition}
    La primitiva $RANDOM(S)$ seleziona
    % in maniera uniforme
    dall'insieme $S$
    un elemento $S_i$
    , estratto uniformemente, con reimbussolamento.
    Invocazioni successive di $RANDOM$ sono indipendenti.
    Nel caso particolare per cui $S = \setzo{}$, la primitiva si dice anche $COIN\_FLIP$.
    \label{def:random}
\end{definition}

Dato un problema $\bpi$, un algoritmo che lo risolve $
A_{\bpi} (i)
$ restituisce ancora una soluzione ammissibile $
s \in \bs(i)
$, ma la soluzione è una variabile aleatoria $
\mathbf{s}
$.

Anche il costo $
c(
\mathbf{s}
)
$ è una variabile aleatoria, di cui si studia il valore atteso: $
\E{ c( \mathbf{s}) }
$.

Il fattore di approssimazione si riscrive come
\begin{equation*}
    \rho
    \leq
    \max 
    \left\{ 
        \frac{
            \E{ c( \mathbf{s}) }
        }{
            c \left( 
                s^*
            \right)
        }
        ,
        \frac{
            c \left( 
                s^*
            \right)
        }{
            \E{ c( \mathbf{s}) }
        }
    \right\}
\end{equation*}
(il primo per problemi di minimo, il secondo per problemi di massimo)

Anche la complessità può essere aleatoria, se il numero di iterazioni è controllato da qualche chiamata a $RANDOM$.

Anche il fattore di approssimazione può essere aleatorio, ma per problemi di massimo in cui la variabile aleatoria è al denominatore, l'analisi è estremamente complessa.

\subsubsection{3-CNF-SAT come problema di ottimizzazione}

Il problema di $3-CNF-SAT$ si pone come problema di ottimizzazione (massimo):

\begin{align*}
    MAX\_3-CNF-SAT: & \\
    \texttt{istanza:} \quad &
    % \langle \Phi \left( x_1, \cdots, x_n \right) \rangle
    \langle \Phi \rangle
    \text{ in 3-CNF} \\
    \text{dove} \quad
    & 
        \Phi \left( x_1, \cdots, x_n \right) = 
        \bigwedge\limits_{j=1}^{m}
        C_j
        \\
    & C_j =y_j^1 \vee y_j^2 \vee y_j^3
    \\
    % & c( \, \vec{b} \, ) = 
    \texttt{costo:} \quad &
    c( \vec{b} ) = 
    \text{ \# clausole vere sotto $\vec{b}$}
    \\
    \texttt{soluzioni ammissibili:} \quad &
    \bs \left( i \right)
    =
    \setzo{n}
    % \\
    % \texttt{soluzione ottima:} \quad &
    % s^* = \argmax 
    % \left\{ 
        % c(s) : s \in
        % \bs \left( i \right)
    % \right\}
% \end{align*}
    \intertext{
        Per una formula soddisfacibile vale $
        c( \vec{b} ) = m
        $.
        Nella versione decisionale:
    }
% \begin{align*}
    D\_3-CNF-SAT: & \\
    \texttt{istanza:} \quad &
    \langle \Phi, k \rangle \\
    \texttt{domanda:} \quad &
    c( \vec{b} ) \geq k ?
\end{align*}

L'algoritmo randomizzato è quello più intuitivo: si costruisce un assegnamento casuale e si conta il numero di clausole vere.

\begin{algorithm}[H]
\caption{3-CNF-SAT randomizzato}\label{alg:3cnfsat_random}
\begin{algorithmic}[1]
    \Procedure{Random\_Approx\_3-CNF-SAT}{$ \langle \Phi \rangle $}
        \State * sia $ \Phi = \Phi \left( x_1, \cdots, x_n \right) \in 3-CNF $ *
        \For{$i \gets 1 $ to $ n $ }
            \State $b_i \gets $ \Call{Random}{$\setzo{}$}
        \EndFor
        \State $ num \gets 0 $
        \For{$j \gets 1 $ to $ m $ }
            \If{$C_j ( \vec{b} ) = 1$}
                \State $ num \gets num + 1 $
            \EndIf
        \EndFor
        \State return $num$
    \EndProcedure
\end{algorithmic}
\end{algorithm}

Per studiare la qualità dell'approssimazione, si calcola il valore atteso del costo ritornato.

Si associa una variabile indicatrice $Y_i$ ad ogni clausola.
\begin{equation*}
    Y_i = 
    \begin{cases}
        0 & C_i ( \vec{b} ) = 0 \\
        1 & C_i ( \vec{b} ) = 1
    \end{cases}
\end{equation*}
Si calcola per prima la probabilità che una clausola sia verificata:
una clausola è la disgiunzione di tre letterali \emph{distinti}, $C_j =y_j^1 \vee y_j^2 \vee y_j^3$, che sono quindi indipendenti (sono assegnati da chiamate diverse di $RANDOM$). Perché una clausola sia falsa, tutti i letterali devono essere falsi:
\begin{align*}
    Prob \left( Y_i = 1 \right)
    = 
    1- Prob \left( Y_i = 0 \right)
    =
    1 - \prod_{j=1}^{3} Prob(y_{j}^{i} = 0)
    = 1 -
    \frac{1}{8}
    =
    \frac{7}{8}
\end{align*}
Il costo corrisponde al numero di variabili a 1:
\begin{equation*}
    Y = num = \sum_{i=1}^{m} Y_i
\end{equation*}
Il suo valore atteso è
\begin{align*}
    \E{ c( \vec{b} ) }
    =
    \E{Y}
    =
    \E{
    \sum_{i=1}^{m} Y_i
    }
    &= 
    \sum_{i=1}^{m}
    \E{Y_i}
    \intertext{
        L'aspettazione della variabile indicatrice è la probabilità che si avveri l'evento (e la variabile assuma valore 1): $
            \E{Y_i} 
            = 
            Prob \left( Y_i = 1 \right)
        $
    }
    &= 
    \sum_{i=1}^{m}
    Prob \left( Y_i = 1 \right)
    \\
    &= 
    \sum_{i=1}^{m}
    \frac{7}{8}
    =
    m
    \frac{7}{8}
\end{align*}
Ricordando che il costo ottimo è maggiorato dal numero di clausole ($
    c( \vec{b}^* )
    \leq m
$), 
il valore per $\rho$ risulta
\begin{equation*}
    \rho
    =
    \frac{
        c( \vec{b}^* )
    }{
        \E{
            c( \vec{b} )
        }
    }
    =
    \frac{
        c( \vec{b}^* )
    }{
        \frac{7}{8}
        m
    }
    \leq
    \frac{
        m
    }{
        \frac{7}{8}
        m
    }
    =
    \frac{8}{7}
\end{equation*}
Scrivere un algoritmo deterministico con questa qualità di approssimazione è possibile, in generale si può sempre de-randomizzare un algoritmo, ma si complica enormemente il codice. Intuitivamente, l'approccio probabilistico gestisce il caso peggiore mostrando che è raro, mentre l'approccio deterministico deve lavorare per evitarlo.

\subsection{Tecnica di arrotondamento}

Si modifica il problema di \emph{vertex cover} introducendo dei pesi per i nodi, e cercando il \emph{vertex cover} di peso minimo.
\begin{align*}
    D\_MW\_VC: & \\
    \texttt{istanza:} \quad & \langle G=(V,E), w, k \rangle \\
    \text{dove} \quad &
    w(v) \text{ pesa i vertici}
    \\
    \texttt{domanda:} \quad &
    \exists \text{ vertex cover } V^* :
    \sum_{v \in V^*} w(v) \leq k
\end{align*}
Il problema si riduce a $VC$ se i pesi sono unitari, e resta quindi $\bnpc$.

Si riscrive come problema di programmazione lineare intera, codificando la selezione dei vertici nel vettore $x$, dove $x_v=1$ se il vertice $v$ è scelto.
\begin{equation*}
    \begin{cases}
        \quad
        \min
        &
        \sum_{v \in V^*} w(v) x_v
        \\
        x_u + x_v \geq 1
        &
        \forall e = \left\{ u,v \right\} \in E
        \\
        x_v \in \setzo{}
        &
        \forall v \in V
    \end{cases}
\end{equation*}
Risolvendo il problema si ricava il \emph{minimal weight vertex cover}
\begin{equation*}
    C^* = \left\{ 
        v \in V : x_v^* = 1
    \right\}
\end{equation*}
infatti vale
\begin{equation*}
    \sum_{v \in V^*} w(v) x_v
    =
    \sum_{v \in C^*} w(v)
\end{equation*}
e il vettore $\vec{x}^*$ minimizza la prima quantità.

La risoluzione del problema di programmazione lineare intera è però comunque NP-hard, quindi si cerca una soluzione approssimata, analizzando il rilassamento continuo del problema, che ammette un solutore polinomiale.
\begin{equation*}
    \begin{cases}
        % \min 
        \quad
        \min 
        &
        \sum_{v \in V^*} w(v) x_v = z^*
        \\
        x_u + x_v \geq 1
        &
        \forall e = \left\{ u,v \right\} \in E
        \\
        0 \leq x_v \leq 1
        &
        \forall v \in V
    \end{cases}
\end{equation*}
Lo spazio delle soluzioni a questo problema è più ampio, quindi il costo individuato è un \emph{lower bound} al costo del \emph{minimal weight vertex cover} ottimo: $z^* \leq w(C^*)$.

Si utilizza una strategia di \emph{rounding}, per cui dalla soluzione $
\vec{x}^*
$ al problema continuo si ricava $
\vec{
    \bar{x}
}^*
$, ammissibile per il problema intero.
\begin{equation*}
    \bar{x}_v
    = 1
    \Leftrightarrow
    x^*_v \geq 0.5
\end{equation*}
Il vettore ricavato è una soluzione ammissibile per il problema intero.
Considerando un generico vincolo per il problema continuo
$ \forall e = \left\{ u,v \right\} \in E $ vale
\begin{align*}
    x_v^*
    +
    x_u^*
    \geq 1
    &
    \Leftrightarrow
    \left( 
        x^*_v \geq 0.5
    \right)
    \vee
    \left( 
        x^*_u \geq 0.5
    \right)
    \\
    &
    \Leftrightarrow
    \left( 
        \bar{x}_v = 1
    \right)
    \vee
    \left( 
        \bar{x}_u = 1
    \right)
\end{align*}

Riguardo alla qualità del \emph{minimal weight vertex cover} $C$ trovato in questo modo, si trova una relazione tra il costo della soluzione del rilassamento continuo e il \emph{vertex cover} ritornato: 
\begin{align*}
    z^*
    &= 
    \sum_{v \in V} w(v) \,
    x_v^*
    % \\
    % \text{(riduce il numero di vertici contati)}
    \intertext{riducendo il numero di vertici contati nella somma}
    % \quad
    &
    \geq
    \sum_{
        v \in V : 
        x_v^*
        \geq
        1/2
    } w(v) \, x_v^*
    % \\
    % \text{($
    % x_v^*
    % \geq 1/2
    % $)}
    % \quad
    \intertext{che sono solo quelli con la variabile associata maggiore o uguale a $1/2$}
    &
    \geq
    \frac{1}{2}
    \sum_{
        v \in V : 
        x_v^*
        \geq
        1/2
    } w(v)
    \\
    &
    =
    \frac{1}{2}
    \sum_{
        v \in V : 
        \bar{x}_v = 1
    } w(v)
    \\
    &
    =
    \frac{1}{2}
    w(C)
\end{align*}
Il costo $z^*$ è un \emph{lower bound} alla soluzione ottima  $w(C^*)$, per cui
\begin{equation*}
    w(C^*)
    \geq
    z^*
    \geq
    \frac{1}{2}
    w(C)
\end{equation*}
E il fattore di approssimazione risulta essere garantito (non è un approccio randomizzato)
\begin{equation*}
    \rho =
    \frac{
        w(C)
    }{
        w(C^*)
    }
    \leq
    \frac{
        w(C)
    }{
        \frac{1}{2}
        w(C)
    }
    = 2
\end{equation*}

\subsection{Randomized rounding}

Una tecnica ibrida estremamente efficace è stata presentata da Raghavan e Tompson in
Randomized Rounding: A Technique for Provably Good Algorithms and Algorithmic Proofs
\cite{Raghavan:1987:RRT:45291.45296}.


\section{Altri risultati}

\subsection{Node cut}
% pag 78

Dato un grafo $
G = (V,E)
$, si cerca una bipartizione dei nodi $
( W, V-W )
$ per cui la capacità (\emph{size}) del taglio è massima:
\begin{equation*}
    S \left( W, V-W \right) = 
    | \left\{ 
        e = \left\{ u,v \right\} \in E : 
        \left( u \in W \right)
        \wedge
        \left( v \in V-W \right)
    \right\} |
\end{equation*}

\subsubsection{Approssimazione deterministica}

L'idea è di partire da una soluzione ammissibile facile e utilizzare una strategia di ricerca locale: ad ogni iterazione si effettua una minima movimentazione della soluzione corrente, migliorandola di volta in volta, fino a trovare un ottimo locale. Chiaramente questo può non coincidere con l'ottimo globale.

\begin{algorithm}[H]
\caption{Approssimatore per max cut}\label{alg:nc_approx}
\begin{algorithmic}[1]
    \Procedure{Approx\_Max\_Cut}{$ G = (V,E) $}
        \State $W \gets V$
        \While{true}
        \State $W2V \gets 
                \left( 
                    * \;
                    \exists v \in W :
                    S \left( W -
                    \{ v \},
                    \left( V-W \right)
                    \cup
                    \{ v \}
                    \right)
                    >
                    S \left( W, V-W \right)
                    \; *
                \right)
            $
        \State $V2W \gets 
                \left( 
                    * \;
                    \exists v \in V-W :
                    S \left( W
                    \cup
                    \{ v \},
                    \left( V-W \right)
                    -
                    \{ v \}
                    \right)
                    >
                    S \left( W, V-W \right)
                    \; *
                \right)
            $
            \If{$
                \left( 
                    W2V
                \right)
                \vee
                \left( 
                    V2W
                \right)
            $}
                \State * sposta il nodo appropriatamente *
            \Else
                \State return $ \left( W, V-W \right) $
            \EndIf
        \EndWhile
    \EndProcedure
\end{algorithmic}
\end{algorithm}

La soluzione ammissibile iniziale è $
W_{init}
= V
$, con $
V-W_{init} = \emptyset
$ e di costo nullo: $
S \left( W_{init} , V- W_{init} \right)
= 0
$.

La cardinalità della soluzione ottima è limitata dal numero di archi:
\begin{equation*}
    |
    W^*
    |
    =
    |
    \argmax_{W \subseteq V}
    \left\{ 
        S \left( W, V-W \right)
    \right\}
    |
    \leq |E|
\end{equation*}
Il valore si raggiunge nel caso di grafo bipartito.

Per quanto riguarda la correttezza, l'algoritmo ritorna di sicuro un taglio, perché ogni bipartizione è un taglio valido.
Inoltre l'algoritmo termina, perché ad ogni iterazione il valore $
S \left( W, V-W \right)
$ cresce monotonicamente (le guardie dell'if hanno un maggiore stretto), il costo inizia da 0 e raggiunge al massimo $|E|$, per cui esegue al massimo $|E|$ iterazioni.

Si dimostra che la cardinalità del taglio ritornato è almeno
$$
S \left( W_f, V-W_f \right)
\geq 
\frac{
    |E|
}{
    2
}
\quad
\to
\quad
\rho
= 
\frac{
S \left( W_f, V-W_f \right)
}{
S \left( W_{init} , V- W_{init} \right)
}
\leq 2
$$
Si analizza il momento in cui l'algoritmo termina: perché questo succeda, non c'è nessun nodo che, se spostato, migliora il costo.
Si indica con $d_v = deg(v)$ il grado di un nodo $v$ e con $t_v$ il numero di nodi adiacenti ma dalla ``parte opposta'' del taglio.
Spostando $v$ il costo varia di $
d_v - 2 t_v
$.
% TODO disegno taglio pag 78.9
I vicini $t_v$ devono quindi essere più della metà, altrimenti spostandolo si otterrebbe un incremento dell'obiettivo. Si dimostra per assurdo: si supponga esista un nodo $
% \exists
v' \in W
$ con $
t_{v'}
<
d_{v'}
/2
$. Il costo risulta
$$
    S \left( W_f -
    \{ v' \},
    \left( V-W_f \right)
    \cup
    \{ v' \}
    \right)
    =
    S \left( W_f, V-W_f \right)
    +
    d_{v'}
    -
    2 t_{v'}
$$
per cui si ha un incremento, ma questo è impossibile perché l'algoritmo avrebbe spostato il nodo.

Vale allora
\begin{align*}
    S \left( W_f, V-W_f \right)
    &= 
    \frac{1}{2}
    \sum_{v \in V} t_v
    \\
    &
    \geq
    \frac{1}{2}
    \sum_{v \in V}
    \frac{
    d_v
    }{2}
    \\
    &= 
    \frac{1}{4}
    \sum_{v \in V} d_v
    \\
    &=
    \frac{1}{4}
    2 |E|
    \\
    &=
    \frac{ |E| }{2}
\end{align*}

\subsubsection{Algoritmo randomizzato}

La stessa qualità di approssimazione si può ottenere con l'algoritmo randomizzato più intuitivo possibile: per ogni nodo si tira una moneta e si decide se selezionarlo in $W$ o meno.
L'insieme è ora una variabile aleatoria, che si indica con $\bw$.

Il fattore di approssimazione risulta
\begin{equation*}
    \rho = 
    \frac{
        S \left( W^*, V-W^* \right)
    }{
        \E{
            S \left( \bw , V- \bw \right)
        }
    }
\end{equation*}
Si associa una variabile indicatrice $X_e$ ad ogni arco $e$, che vale 1 se è un arco del taglio.
\begin{equation*}
    X_e = 1 \Leftrightarrow
    e = \left\{ u,v \right\}
    :
    \left( 
        \left( u \in \bw \right)
        \wedge
        \left( v \in V - \bw \right)
    \right)
    \vee
    \left( 
        \left( u \in V - \bw \right)
        \wedge
        \left( v \in \bw \right)
    \right)
\end{equation*}
I due eventi sono disgiungi, quindi
la probabilità che una specifica variabile indicatrice (fissato $e=\{u,v\}$) sia uno è pari a
\begin{align*}
    Prob \left( X_e = 1 \right)
    &= 
    Prob \left( 
        \left( u \in \bw \right)
        \wedge
        \left( v \in V - \bw \right)
    \right)
    \\
    &+
    Prob \left( 
        \left( u \in V - \bw \right)
        \wedge
        \left( v \in \bw \right)
    \right)
    \\
    &= 
    \frac{1}{2}
    \frac{1}{2}
    +
    \frac{1}{2}
    \frac{1}{2}
    =
    \frac{1}{2}
\end{align*}
Il costo si riscrive come
\begin{equation*}
    S \left( \bw , V- \bw \right)
    =
    \sum_{e \in E} X_e
\end{equation*}
E il suo valore atteso è
\begin{align*}
    % \E{
        % S \left( \bw , V- \bw \right)
    % }
    % =
    \E{
        \sum_{e \in E} X_e
    }
    =
    \sum_{e \in E}
        \E{
            X_e
        }
    =
    \sum_{e \in E}
    Prob \left( X_e = 1 \right)
    = \frac{|E|}{2}
\end{align*}
Il costo della soluzione ottima è limitato da $|E|$, quindi l'algoritmo è di 2-approssimazione.

\subsection{Non esistenza di algoritmo FPTAS per Triangle-TSP}
% pag 79.8

Se $\bp = \bnp$ sarebbe disponibile la soluzione esatta.

Sotto l'ipotesi $\bp \neq \bnp$, non può esistere un algoritmo FPTAS che risolva il problema di $Triangle\_TSP$, altrimenti si potrebbe esibire un decisore polinomiale per il problame di $HAMILTON$.

Si riduce da $HAMILTON \lp T\_TSP$
\begin{equation*}
    f :
    \langle
        G = (V,E)
    \rangle
    \to
    \langle
        G^K = (V,E^K), c
    \rangle
\end{equation*}
dove il grafo $G^K$ è il grafo completato, e i costi sono associati agli archi nella maniera seguente:
\begin{equation*}
     c(e) = 
    \begin{cases}
        1 & e \in E \\
        2 & e \notin E
    \end{cases}
\end{equation*}
Questa pesatura verifica la disuguaglianza triangolare: le possibili somme del costo di due archi sono
% $
(1+1),
(1+2),
(2+2)
% $
e i costi del singolo arco sono $
1,2
$, ovvero sempre minori o uguali delle somme.

Si supponga esista uno schema di approssimazione pienamente polinomiale che risolve il problema $
A_{TT} \left( 
    \langle
        G^K = (V,E^K), c
    \rangle
    , \varepsilon
\right)
$ che ritorna un ciclo $C$, è polinomiale sia nella taglia dell'istanza $|
    \langle
        G, c
    \rangle
|$ sia in $1/\varepsilon$, con fattore di approssimazione pari a
\begin{equation*}
    \rho = 
    \frac{
        c \left( C \right)
    }{
        c \left( C^* \right)
    }
    \leq \left( 1+\varepsilon \right)
\end{equation*}
Rendendo $\varepsilon$ inversamente proporzionale alla taglia $G$, si ottiene un algoritmo che è ancora polinomiale, ma che compie un errore così piccolo da disabilitare l'approssimazione.
\begin{equation*}
    \varepsilon = 
    \frac{1}{2|V|}
    \quad
    \to
    \quad
    A_{TT} \left( 
        \langle
            G^K = (V,E^K), c
        \rangle
        ,
        \frac{1}{2|V|}
    \right)
\end{equation*}
Lo schema FPTAS è polinomiale in $1/\varepsilon$ per cui risulta $poly(|V|) = poly (|
\langle
G,c
\rangle 
|)
$.
\\
Nel primo caso, se $ \langle G = (V,E) \rangle \in HAMILTON$, esiste in $G$ un cammino Hamiltoniano, che esiste anche in $G^K$ e ha costo $|V|$, perché tutti gli archi utilizzati sono originali.
L'algoritmo ritorna un ciclo $C$ il cui costo è
\begin{align*}
    c(C) 
    &
    \leq
    \left( 1 + \varepsilon \right)
    c(C^*)
    % \\
    % &= 
    =
    \left( 1 +
        \frac{1}{2|V|}
    \right)
    |V|
    = |V| + \frac{1}{2}
\end{align*}
I costi possono solo essere interi, per cui 
$
c(C) \leq |V|
$.
\\
Nel caso di istanza negativa, $
\langle G = (V,E) \rangle \notin HAMILTON
$ implica che ogni tour $C$ in $G^K$ deve usare almeno un arco in $E^K-E$.
$A_{TT}$ ritorna quindi un tour di costo 
$
c(C) \geq |V| + 1
$, dovendo per forza utilizzare un arco di costo 2.
\\
Combinando i risultati:
\begin{equation*}
    \langle G \rangle \in HAMILTON
    \Leftrightarrow
    A_{TT}
    \left( 
        f ( \langle G \rangle)
        ,
        \frac{1}{2|V|}
    \right)
    \text{ ritorna un tour di costo } c(C) \leq |V|
\end{equation*}
Si riesce quindi a decidere $HAMILTON$ in tempo polinomiale, che viola l'ipotesi che si era fatta per cui
$\bp \neq \bnp$.

\subsection{Soluzione approssimata per Subset Sum}
% pag 80.8

L'algoritmo FPTAS ricavato per Subset Sum alla sezione \ref{sec:ss_fptas} ha complessità cubica nella taglia dell'istanza. Se si ammette una soluzione approssimata, si può sviluppare un algoritmo con approccio greedy di complessità lineare.

L'idea è di selezionare elementi fino a superare il target.

Un esempio suggerisce in che modo conviene ordinare l'insieme: sia $
S = \left\{ 1, 2, t-2 \right\}
$ con ordine crescente, la soluzione è $S' = \left\{ 1, 2 \right\}$ di costo 3, che è pessima.
L'ordinamento decrescente è evidentemente più efficace: $
S = \left\{ t-2, 2, 1 \right\}
$ seleziona $
S' = \left\{ t-2, 2 \right\}
$ che somma addirittura al target esatto.
\begin{algorithm}[H]
\caption{Approssimatore greedy per Subset Sum}\label{alg:ss_approx_greedy}
\begin{algorithmic}[1]
    \Procedure{Approx\_SS}{$ \langle S, t \rangle $}
        \State $\langle x_1, \cdots, x_n \rangle \gets \Call{Sort\_Decr}{S} $
        \State $S' \gets \left\{ x_1 \right\}$
        \State $sum \gets x_1$
        \For{$i \gets 2 $ to $ n $ }
            \If{$ sum + x_i \leq t$}
                \State $S' \gets S' \cup \left\{ x_i \right\}$
                \State $sum \gets sum + x_i$
            \Else
                \State return $S', sum$
            \EndIf
        \EndFor
    \EndProcedure
\end{algorithmic}
\end{algorithm}
L'algoritmo termina appena la somma supera il target. La qualità della soluzione sarebbe chiaramente maggiore se venissero selezionati altri elementi successivi, e quindi minori, di quello che sfora, ma la garanzia dell'approssimazione non migliora.

La complessità è lineare, l'algoritmo è corretto (soluzione ammissibile) perché si ferma proprio quando la somma supera il target.

Per quanto riguarda il fattore di approssimazione, ci sono due casi.
Nel primo, il ciclo esaurisce tutti gli elementi nell'insieme: allora $S' = S$ e anche $S' = S^*$, perché non si può fare meglio di così.
Nel secondo caso, il ciclo si arresta durante l'iterazione $
\bar{i}
$, con $
2 \leq \bar{i} \leq n
$. L'insieme ritornato è $
S' = \left\{ x_1, x_2, \cdots,
x_{\bar{i}-1}
\right\}
$, tale che
\begin{equation*}
    \sum_{s \in S'} s
    =
    sum
    \leq t
    \quad
    \text{e}
    \quad
    sum + 
    x_{\bar{i}-1}
    > t
\end{equation*}
Vale la catena
\begin{align*}
    sum
    &
    \geq
    x_1
    &
    \text{lo contiene di sicuro}
    \\
    &
    >
    x_{\bar{i}}
    &
    \text{per l'ordinamento}
    \\
    &
    >
    t - sum
    &
    \text{condizione per l'uscita del ciclo}
\end{align*}
Da cui
$$
2 \, sum > t
\Rightarrow
sum > \frac{t}{2}
$$
E il fattore di approssimazione risulta
\begin{equation*}
    \rho = 
    \frac{
        % \displaystyle
        \sum_{s \in S^*} s
    }{
        % \displaystyle
        \sum_{s \in S'} s
    }
    <
    \frac{t}{t/2}
    = 2
\end{equation*}
Il fattore si può stringere lievemente, sfruttando le disequazioni strette nella catena per cui $
a>b \to a \geq b + 1
$
\begin{align*}
    sum
    &
    \geq
    x_1
    \\
    &
    >
    x_{\bar{i}}
    \geq
    x_{\bar{i}} + 1
    \\
    &
    >
    (t - sum) + 1
    \geq 
    (t - sum + 1) + 1
\end{align*}
E continuando il ragionamento come sopra risulta 
\begin{equation*}
    sum \geq \frac{t+2}{t}
    = \frac{t}{2} + 1
\end{equation*}
con un fattore di approssimazione
\begin{equation*}
    \rho = 
    <
    \frac{t}{t/2+1}
    =
    \frac{2t}{t+2}
\end{equation*}
Questa è l'analisi più stretta possibile, infatti per $
S = \left\{ 4, 3, 2, 1 \right\}, t = 6
$ viene ritornato $
S' = \left\{ 4 \right\}
$ a fronte di una soluzione ottima che somma a 6, per cui
\begin{equation*}
    \rho = \frac{6}{4}
    = 
    \frac{2t}{t+2}
\end{equation*}

\subsection{Soluzione approssimata per Independent Set}
% pag 81

Il problema dell'Independent Set richiede di trovare l'insieme di taglia massima di nodi non connessi tra loro.
L'approccio greedy intuitivo è quello di selezionare un nodo nell'$IS$ e rimuovere da una lista di nodi liberi tutti i suoi vicini.
\begin{algorithm}[H]
\caption{Approssimatore per Independent Set}\label{alg:is_approx}
\begin{algorithmic}[1]
    \Procedure{Approx\_IS}{$G=(V,E)$}
        \State $V' \gets \emptyset$
        \Comment{vertici selezionati}
        \State $W \gets V$
        \Comment{vertici residui}
        \While{$W \ne \emptyset$}
            \State * sia $v \in W$ arbitrario *
            \State $V' \gets V' \cup \left\{ v \right\}$
            \State $W \gets W - \left\{ v \right\}
            - \left\{ u \in W : \left\{ u,v \right\} \in E \right\}
            $
        \EndWhile
        \State return $s$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
L'algortimo è corretto e ritorna un independent set: perché non lo sia devono essere selezionati due nodi connessi da un arco: $
\exists u, v \in V' :
\left\{ u, v \right\} \in E
$, ma questo non può accadere, 
senza perdita di generalità, sia $u$ il primo nodo selezionato, allora nel clean up vengono rimossi da $W$ sia $v$ sia $u$, quindi si arriva all'assurdo: $v$ non può essere selezionato insieme a $u$.

L'algoritmo ritorna un independent set massimale, ovvero non si può aggiungere un nodo a $V'$ facendolo restare un $IS$.
Per ogni $u \notin V'$, se non è stato selezionato deve essere stato eliminato a una qualche iterazione. Ma non è in $V'$ quindi viene rimosso nel clean up, per cui deve avere un vicino $v \in V'$, e $u$ viene eliminato all'iterazione a cui viene selezioanto $v$.

Per quanto riguarda il fattore di approssimazione,
si cerca di capire quanti più nodi abbia l'insieme ottimo rispetto al ritornato.
Si considerino l'insieme
che viene ritornato
$V'$,
l'insieme ottimo 
$V^*$,
la loro differenza $
V^* - V'
$
e la loro intersezione
$X = V^* \cap V'$.

L'idea è che $V^*$ non possa avere troppi più nodi rispetto a $V'$,
e sono i nodi nella differenza $
V^* - V'
$.

---------

I nodi che sono in $V^*$ ma non sono in $V'$, devono essere adiacenti a nodi di $V'$ grazie alla massimalità.

Ogni nodo di $V'$ può coprire al più $\Delta_G$ adiacenze, e il suo grado è minore di $\Delta_G$.

Quindi non c'è troppo spazio per ingrandire $V^*$, al più circa $\Delta_G |V'|$ 

---------

Considerando un vertice generico $v \in V^*$ ci sono due casi:
$v \in V'$ in cui il vertice è individuato correttamente, e 
$v \notin V'$.
In questo secondo caso, $v \in V^* - V'$, e per la massimalità di $V'$ ogni nodo nella differenza deve avere un vicino in $V'$, ovvero $
\exists u \in V' :
\left\{ u, v \right\} \in E
$.

Allora vale $
|V^*| = |X| + |V^* - V'|
$, infatti $V^*$ ha nodi che sono o in $X$ o nella differenza.

$X$ partiziona $V^*$.

Ogni nodo in $V^* - X$ deve essere adiacente a un nodo nell'insieme $
V' - X
$, perché non può essere adiacente a un nodo in $X$, che sono anche nodi in $V'$,
e si trovano nello stesso independent set $V^*$ e non ci sono archi tra quei nodi.

Tutti i nodi che non sono in $V'$, per massimalità, devono essere adiacenti a un nodo in $V'$ che non è in $X$, e quindi $V' - X$.

Quanti nodi si possono accomodare? Al più $\Delta_G $ per ognuno di quei nodi, e quindi vale la diseguaglianza
\begin{equation*}
    | V^* - X |
    \leq
    \Delta_G \left( |V'| - |X| \right)
\end{equation*}
E $
| V^* - X |
=
| V^* | - | X |
$ essendo la differenza simmetrica, ho tolto da $V^*$ proprio i nodi che sono in $V'$.

Perché si toglie $X$ da questa quantità? Perché ogni nodo non in $V'$, deve essere adiacente a un nodo di $V'$, ma non possono essere tutti i nodi di $V'$ ad accomodare nodi di $X$, perché $X$ è un insieme di nodi che si trova anche in $V'$. Quindi i nodi in $X$ non possono servire per avere un vicino in $V'$.
I nodi in $X$ sono in $V^*$ e due nodi in $V^*$ non sono uniti da un arco.
Ho solo $|V'|-|X|$ nodi da utilizzare per ottenere queste adiacenze.

% Per ciascuno di questi nodi $v$ di $
% V^* - X
% =
% V^* -
% \left( 
    % V^* \cap V'
% \right)
% $, ovvero nodi in $V^*$ ma non in $V'$,
% possono essere selezionati in $V'$ ???

\begin{align*}
    |V^*|
    &= 
    |X|
    +
    |V^* - X|
    \\
    &
    \leq
    |X| + 
    \Delta_G \left( 
        |V'| - |X|
    \right)
    % \\
    \intertext{supponendo che il grado massimo sia almeno 1 (grafo non isolato)}
    &
    \leq
    \Delta_G \left( 
        |V'| -
        |X| + 
        |X|
    \right)
    =
    \Delta_G |V'|
\end{align*}

Ragionamento di corrispondenza, si prendono i nodi della soluzione ottima, e li si fanno corrispondere a nodi della soluzione approssimata, sapendo che i nodi della soluzione ottima che non sono nella soluzione approssimata, ad al più gruppi di $\Delta_G$ possono corrispondere allo stesso nodo nella soluzione approssimata che non si trovi anche nella soluzione ottima.

% TODO info min 23 su come migliorare l'approx

In pratica l'independent set che si ottiene sono molto dipendenti dall'ordine di selezione dei nodi (esempio stella).
% In generale, sotto una permutazione casuale dei nodi, il centro verrebbe selezionato  
Permutando gli ingressi si ottengono qualità diverse della soluzione finale: non si riesce a migliorare il caso peggiore, ma provando più permutazione di nodi si possono ottenere a volte buone soluzioni.

Tipicamente si usa un algoritmo di approssimazione per ottenere una prima soluzione con qualità garantita.
Poi si provano tutti i modi per migliorarlo:
\begin{itemize}[noitemsep,parsep=0pt,partopsep=0pt,topsep=0pt]
    \item con tecniche di ricerca operativa (branch and bound)
    \item con diverse run randomizzate
    \item con local search, si ottiene buona soluzione iniziale, e poi si effettuano piccole perturbazioni alla soluzione ottenuta, per cercare di migliorare la soluzione fino a trovare un ottimo locale.
\end{itemize}

\section{Problema del Bilanciamento del Carico}
% pag 82.5

In un problema di \emph{load balancing}, vanno eseguiti $n$ job (indicati con numeri naturali $\left\{ 1, \cdots, n \right\}$,
ciascuno con running time noto $t_1, \cdots, t_n$, su $m$ macchine, con $m < n$ perché non sia triviale.

Le soluzioni ammissibili sono partizioni degli indici dei job: 
\begin{equation*}
    \mathcal{P} = 
    \left\{ 
        M_1, \cdots, M_m
    \right\}
    \quad
    M_i \subseteq \left\{ 1, \cdots, n \right\}
    , \;
    M_i \cap M_j = \emptyset
    , \;
    \bigcup_{i = 1}^{m} M_i
    = \left\{ 1, \cdots, n \right\}
\end{equation*}
Il tempo per ogni macchina è
\begin{equation*}
    T_i = 
    \sum_{j \in M_i} t_j
\end{equation*}
La funzione obiettivo è il cosiddetto \emph{makespan}, ovvero il tempo impiegato dalla macchina più carica:
\begin{equation*}
    % \max_{i=1, \ldots, m} T_i
    \max
    \left\{ 
        T_i : i=1, \ldots, m
    \right\}
\end{equation*}
che deve essere minimizzato.

Per ESERCIZIO si può provare che anche per due sole macchine, questo problema è NP-hard, con una riduzione non banale da subset sum, seguendo l'idea che partizionando l'insieme in due metà le somme devono essere simili.

Un immediato algoritmo greedy di approssimazione assegna man mano i job alla macchina più scarica.
\begin{algorithm}[H]
\caption{Approssimatore per load balancing}\label{alg:lb_approx}
\begin{algorithmic}[1]
    \Procedure{Approx\_LB}{$\vec{t}, m$}
        \State $n \gets \vec{t}.len$
        \For{$i \gets 1 $ to $ m $ }
            \State $T_i \gets 0$
            \State $M_i \gets \emptyset$
        \EndFor
        \For{$j \gets 1 $ to $ n $ }
            \State $k \gets \argmin \left\{ T_i : 1 \leq i \leq m \right\}$
            \State $T_k \gets T_k + t_j$
            \State $M_k \gets M_k \cup \left\{ j \right\}$
        \EndFor
        \State return $\max ( \vec{T} ), \vec{M}$
    \EndProcedure
\end{algorithmic}
\end{algorithm}
La soluzione ritornata è di sicuro ammissibile, perché ogni job viene assegnato una e una sola volta. Con un implementazione astuta delle strutture dati si ottiene complessità lineare (bisogna avere sempre disponibile l'indice della macchina più scarica).

Per poter dimostrare il fattore di approssimazione, bisogna prima mostrare una proprietà del makespan $T^*$ della soluzione ottima:
\begin{equation*}
    T^* \geq
    \max
    \left\{ 
        \frac{1}{m}
        \sum_{j=1}^{n} t_j
        ,
        \max_{1 \leq j \leq n} t_j
    \right\}
\end{equation*}
Ovvero il massimo tra il lavoro medio per macchina, e il costo del lavoro più pesante.
\\
La prima proprietà si prova per assurdo: se 
\begin{equation*}
    T^* <
    \frac{1}{m}
    \sum_{j=1}^{n} t_j
\end{equation*}
tutti i tempi devono essere minori di questo ($T^*$ è il massimo)
\begin{equation*}
    \forall i \quad
    T_{i}^{*} \leq
    T^* <
    \frac{1}{m}
    \sum_{j=1}^{n} t_j
\end{equation*}
che per pigeon hole è assurdo: sommando tutti gli $m$ contributi, si ottiene
\begin{equation*}
    \sum_{i=1}^{m}
    T_{i}^{*}
    <
    m
    \frac{1}{m}
    \sum_{j=1}^{n} t_j
\end{equation*}
E il primo membro si può riscrivere come il secondo per l'associativa
\begin{equation*}
    \sum_{j=1}^{n} t_j
    =
    \sum_{i=1}^{m} T_i
\end{equation*}
Quindi si è ottenuto che una quantità deve essere minore di sé stessa, che è impossibile.
\\
Per la seconda proprietà, sia
$
\bar{j} = \argmax \left\{ t_j : 1 \leq j \leq m \right\}
$
l'indice del job più lungo. Deve essere eseguito da una macchina
$
\exists i : \bar{j} \in M_{i}^{*}
$, e il tempo complessivo della macchina sarà di sicuro maggiore o uguale al singolo job
$
T_{i}^{*}
\geq
t_{\bar{j}}
$, e il makespan è il massimo dei tempi delle macchine
$
T^* \geq
T_{i}^{*}
$

Per il fattore di approssimazione, si analizza la macchina critica $\bar{i}$, con makespan:
\begin{equation*}
    T^G = 
    \max_{1 \leq j \leq n} \left\{ T_i^G \right\}
    =
    T_{\bar{i}}^{G}
\end{equation*}
e ne si studia la storia delle allocazioni, in particolare l'ultima: si indica con $\bar{j}$ l'ultimo job assegnato a $\bar{i}$.
A questa iterazione, le altre macchine erano più cariche:
\begin{equation*}
    \forall i
    ,
    1 \leq i \leq m
    \quad
    T_{i}^{G}
    \geq
    \left( 
        T_{\bar{i}}^{G}
        -
        t_{\bar{j}}
    \right)
    =
    \left( 
        T^{G}
        -
        t_{\bar{j}}
    \right)
    % \geq
    % \left( 
        % T^*
        % -
        % t_{\bar{j}}
    % \right)
\end{equation*}
Sfruttando quanto mostrato prima
($
    m \, T^*
    \geq
    \sum_{j=1}^{n} t_j
$), e sommando per tutte le macchine
\begin{align*}
    &
    \sum_{i=1}^{m}
    T_{i}^{G}
    =
    \sum_{j=1}^{n} t_j
    \leq
    m \, T^*
    \\
    m \left( 
        T^{G}
        -
        t_{\bar{j}}
    \right)
    \leq
    &
\end{align*}
Da cui
\begin{align*}
    T^G
    &
    \leq
    T^* -
    t_{\bar{j}}
    \\
    & \leq
    T^* +
    \max_{1 \leq j \leq n}
    \left\{ 
        t_j
    \right\}
    \\
    & \leq
    T^* +
    T^*
    = 2 
    T^*
\end{align*}
E il fattore di approssimazione è
\begin{equation*}
    \rho = \frac{
        T^G
    }{
        T^*
    }
    \leq 2
\end{equation*}

\subsubsection{Considerazioni}

La qualità dell'analisi è buona per grandi valori di $m$:
se si considerano $
m(m-1)
$ job con costo 1, e l'ultimo job con costo $
t_n = m
$, con $
n = 
m(m-1) + 1
$, i primi $n-1$ job vengono allocati in ordine uno per macchina, e all'ultima iterazione hanno tutte lo stesso carico pari a $m-1$.
L'ultimo job viene assegnato a una macchina qualsiasi, che porta a un makespan pari a $
T_G = (m-1)+m = 2m -1
$, mentre $T^* = m$, assegnando ad una macchina solamente il job lungo, e alle restanti $m-1$ tutti gli altri job.
\begin{equation*}
    \rho = \frac{
        T^G
    }{
        T^*
    }
    = \frac{2m-1}{m}
    = 2- \frac{1}{m}
\end{equation*}
Seguendo l'euristica \emph{longest processing time first}, si ottiene un fattore di approssimazione pari a 
\begin{equation*}
    \rho_{LPFT} = \frac{3}{2}
\end{equation*}
Si dimostra soffermandosi sul costo del $m+1$-esimo job assegnato, che si dimostra essere
\begin{equation*}
    T^* \geq 2 t_{m+1}
\end{equation*}
Intuitivamente, se ci sono più di $m$ job, una macchina deve farne due.

